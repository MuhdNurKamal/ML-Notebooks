{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06 » Regularisation and Validation » CS3244 Machine Learning ",
      "provenance": [],
      "collapsed_sections": [
        "0tOR3LsKl7k6",
        "1ekIF5ue4KUT",
        "6WBjlvLOOSB7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgR2VTEuTi3y",
        "colab_type": "text"
      },
      "source": [
        "Available at http://www.comp.nus.edu.sg/~cs3244/1910/06.colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvf81x6Vrysn",
        "colab_type": "text"
      },
      "source": [
        "![Machine Learning](https://www.comp.nus.edu.sg/~cs3244/1910/img/banner-1910.png)\n",
        "---\n",
        "See **Credits** below for acknowledgements and rights.  For NUS class credit, you'll need to do the corresponding _Assessment_ in [CS3244 in Coursemology](http://coursemology.org/courses/1677) by the respective deadline (as in Coursemology). \n",
        "\n",
        "**You must acknowledge that your submitted Assessment is your independent work, see questions in the Assessment at the end.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9DwFQGYq0xV",
        "colab_type": "text"
      },
      "source": [
        "**Learning Outcomes for this Week**\n",
        "- Understand Regularization as a means of restraining the model.\n",
        "- Choose appropriate doses of regularization for a model: an appropriate regularization function and coefficient.\n",
        "- Implement a Ridge Regression from scratch.\n",
        "- Understand and be able to execute validation as well as common pitfalls in using validation incorrectly.\n",
        "- Understand the different forms extending validation to encompass additional estimation.\n",
        "- Understand how validation and regularization complement each other and their roles in affecting learning\n",
        "- Learn about regularization for image denoising."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMf7rbVqDWJ5",
        "colab_type": "text"
      },
      "source": [
        "_Welcome to the Week 06 Python notebook._ This week we will learn  how regularization is used in machine learning.  We introduce **Regularization** and **Validation** in the lecture videos, and will be reviewing this material in the fifth tutorial.\n",
        "\n",
        "In this notebook, we will go through the concepts of Regularization and Validation. We will also go through a few simple coding example involving these two concepts. We will learn different regularization techniques and amount of regularization to choose. For validation, we will go through $k$-fold cross validation and some of its examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qhsIzZPGYT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Week 06: Pre-tutorial Work\n",
        "\n",
        "* Watch the CS 3244 video playlist for Week 06 Pre.  This will introduce two main concepts for this week's class: _Regularisation_ and _Validation_. These concepts are often used to combat overfitting, and hence essential to machine learning.\n",
        "* After watching the videos, complete the pre-tutorial exercises and questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYKB8b5ZPHLB",
        "colab_type": "text"
      },
      "source": [
        "## 1 Recall Week 05's topics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sbc0mK2PPat",
        "colab_type": "text"
      },
      "source": [
        "In last week's lecture, we learn about overfitting. which happens when we try to fit the data more than warranted. It happens because the hypothesis redundantly models too much **stochastic noise** and **deterministic noise** (bias). The result is that we may end up with a sizeable gap between the training cost ($J_{train}$) and the testing cost ($J_{test}$).  This is bad, because we need to have a reliable estimate of the test cost on new, unseen samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D0h-klK2r4R",
        "colab_type": "text"
      },
      "source": [
        "Roughly speaking: $J_{test}(h)$ = $J_{train}(h)$ + overfit_penalty. \n",
        "\n",
        "Today we will learn about two main approaches used to combat overfitting:\n",
        "\n",
        "1. **Regularization**: Restrain the model capacity, which comes with an additional regularization error cost. This cost is supposed to approximate the overfit penalty, which approximates $J_{test}(h)$ to $J_{train}(h)$.\n",
        "\n",
        "2. **Validation**: Reality check by peeking at a validation dataset to guess $J_{test}(h)$, mainly used to tune the hyperparameters of $J_{train}(h)$ to approximates $J_{test}(h)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFQ5yw1v0pc",
        "colab_type": "text"
      },
      "source": [
        "## 2 Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWDGBccwjhWO",
        "colab_type": "text"
      },
      "source": [
        "### .a Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcIx0yTo9Ats",
        "colab_type": "text"
      },
      "source": [
        "**What is it**: A cure for our tendency to fit (get distracted\n",
        "by) the noise, hence improving $J_{test}$\n",
        "\n",
        "**How does it work**: Constraining the model so that we\n",
        "do not fit the noise. This will reduce the variance in the model (refer to figure below). The constraints also introduce additional value to the training errors, which helps approximating the test error as well.\n",
        "\n",
        "**Side effects**: If we cannot fit the noise, maybe we cannot\n",
        "fit the signal $f$.\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"http://www.comp.nus.edu.sg/~cs3244/1810/6pre_reg_eg.png\" width=512> \n",
        "</div>\n",
        "\n",
        " (Diagram Credit: Modified from the _Learning From Data_ textbook, by Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60exeJZ3jdTW",
        "colab_type": "text"
      },
      "source": [
        "### .b Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PXicjymBYfg",
        "colab_type": "text"
      },
      "source": [
        "Say we have two polynomial models:\n",
        "\n",
        "\n",
        "$\\mathcal{H}_{10} = \\theta_0 + \\theta_1  x + ... + \\theta_{10}  x^{10} $\n",
        "\n",
        "$\\mathcal{H}_2 = \\theta_0 +\\theta_1  x + ... + \\theta_{10} x^{10}$, such that $\\theta_3 = \\theta_4 = ... = \\theta_{10} = 0$. This is known as **\"hard\" order constraints** that set weights to $0$.\n",
        "\n",
        "Instead of setting the weights to zeros explicitly, we should give them a budget and let the learning algorithm choose, e.g. we can set the sum of square of weights to be smaller than a threshold. This is called **\"soft\" order constraints**.\n",
        "\n",
        "\n",
        "$\\mathcal{H}_C = \\theta_0 + \\theta_1 x + ... + \\theta_{10}  \\theta^{10}$, such that $\\sum_{i=0}^{10}(\\theta_i^2) \\leq C$. \n",
        "\n",
        "If we represent $\\theta$ as the weight vector of the weights: $\\theta = (\\theta_0, \\theta_1, ... , \\theta_{10})$, then the constraints become $|\\theta|_2 \\leq C$.\n",
        "\n",
        "However, by having a separate constraint like this, optimization will become much more difficult. Instead, we can move to implicit  constraints where we try to minimize: $J_{train}(\\theta) + \n",
        "\\frac{\\lambda}{m} * ||\\theta||_2.$ \n",
        "\n",
        "\n",
        "($||\\theta||_2$ is the Euclidean (L2) norm of the weight vector)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R39dIinwJe-Z",
        "colab_type": "text"
      },
      "source": [
        "## 3 Validation and Model Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feWYp2zSeBsc",
        "colab_type": "text"
      },
      "source": [
        "In order to decide the optimal $\\lambda$ for regularization we need to have a peek into $J_{test}(w)$. However, the learning algorithm is not allowed to see the test dataset, therefore we will further subdivide among our training dataset into two subsets, one used for actual training (the training set) and the other for estimating its test error, called the validation set.\n",
        "\n",
        "The introduction of the validation set is to help the model have a fair try at some unseen test data for us to estimate its actual test performance, and it is useful for tuning. The division of the dataset can be done randomly, however in some cases, a random division is an incorrect approach. Figure 2 explains how the validation set is used to choose the best model among $\\mathcal{H}_1, \\mathcal{H}_2, ... , \\mathcal{H}_m$\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"http://www.comp.nus.edu.sg/~cs3244/1810/6pre_modelSel.png\" width=400> \n",
        " </div>\n",
        " (Diagram Credit: Modified from the _Learning From Data_ textbook, by Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin)\n",
        " \n",
        " Here we will use $D_{train}$   to train $\\mathcal{H}_1, \\mathcal{H}_2,..., \\mathcal{H}_\\Theta$ to get $h_1^-, h_2^-,..., h_\\Theta^-$. The performance of these models on $D_{val}$ will be used to choose the best one for testing.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tOR3LsKl7k6",
        "colab_type": "text"
      },
      "source": [
        "### .a Check Your Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ipsK72jXhDI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        " <div align=\"center\">\n",
        "  <img src=\"http://www.comp.nus.edu.sg/~cs3244/1810/6pre_learningCurve.png\" width=300> \n",
        "  </div>\n",
        " (Diagram Credit: Modified from the _Learning From Data_ textbook, by Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin)\n",
        "  \n",
        "  In this figure, one line represents $J_{val}(h_\\theta*)$ while the other represents $J_{test}(h_\\theta*)$.\n",
        "  Please answer Question 1–3 accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F58Mg2TmAjZ",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 1):** Which curve represents $J_{val}$?\n",
        "\n",
        "_Choose from: Blue, Red_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaQfdNfvmQw7",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 2):** Why are both curves going up?\n",
        "\n",
        "_Choose from: Validation set size increases, Validation set size decreases, Training set size increases, Training set size decreases_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mELC4ob0mvq8",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 3):** Why do the curves get closer together?\n",
        "\n",
        "_Choose from: Validation set size increases, Validation set size decreases, Training set size increases, Training set size decreases_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sPJkK8tepb5",
        "colab_type": "text"
      },
      "source": [
        "###.b Programming : Validating Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPca5_NReucd",
        "colab_type": "text"
      },
      "source": [
        "Here we will make use of libraries to show how we perform validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CojU1QkaM44O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We start by loading some standard libraries\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmLDhhz9M6pb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "One of the most important pieces of machine learning is model validation: that is, checking how well your model fits a given dataset. But there are some pitfalls you need to watch out for.\n",
        "\n",
        "Consider the digits example we've been looking at in a few of our sessions. \n",
        "\n",
        "How might we check how well our model fits the data?\n",
        "\n",
        "Let's use $k$ nearest neighbors classifier ($k$NN) and try to fit the digits data to that classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsvwh3OeNHYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use seaborn for plotting defaults\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "# Load Digits Dataset\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "# train the classifier using digits data\n",
        "knn.fit(X, y)\n",
        "\n",
        "# get prediction\n",
        "y_pred = knn.predict(X)\n",
        "\n",
        "# print number of correctly classified example among all example\n",
        "print(\"{0} out of {1} correct\".format(np.sum(y == y_pred), len(y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zPbv7PeN4S3",
        "colab_type": "text"
      },
      "source": [
        "Looks like we have a perfect classifier!  Yay ...?\n",
        "\n",
        "Stop and think for a moment - what did we do wrong here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKLQJB8wotOd",
        "colab_type": "text"
      },
      "source": [
        "**Yout Turn (Question 4):** What's wrong with this result?\n",
        "\n",
        "_Choose from: $k$NN cannot be used for this problem, There is nothing wrong with having a perfect result, The learning algorithm has overfitted to our training data_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsjhkoaroeiv",
        "colab_type": "text"
      },
      "source": [
        "### .c Programming : Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMT6njLs9YgB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "In the previous code segment, we used same dataset for training and testing. This is **not a good idea**. If we optimize our estimator this way, we will tend to **overfit** the data, *i.e.* we learn the noise.\n",
        "\n",
        "A better way to test a model is to use a hold-out set which doesn't enter the training. We've seen this before using `scikit-learn`'s train/test split utility. Note that by default, the train/test split method provided splits $25\\%$ of the data to be the test data and $75\\%$ to be the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAmqwT0J9YgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split # import the functionality\n",
        "\n",
        "# split the data set into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "print(\"Training size : {},  Test size: {}\".format(len(X_train), len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0W62VVK9YgI",
        "colab_type": "text"
      },
      "source": [
        "Now we use the same $k$NN classifier, but this time we train and test on different data samples. Let's see how $k$NN classifier works in this setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liuRi3qG9YgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a knn classifier instance\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "# Train the classifier using X_train\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Get prediction for X_test\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"{0} out of {1} correct\".format(np.sum(y_test == y_pred), len(y_test)))\n",
        "# Get accuracy\n",
        "print(\"Accuracy : %.5f\\n\"%(knn.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSa8rxjM9YgM",
        "colab_type": "text"
      },
      "source": [
        "This gives us a more reliable estimate of how our model is doing. Because we are testing our model on unknown data samples, rather than using the same known data samples.\n",
        "\n",
        "The metric we're using here, comparing the number of matches to the total number of samples, is known as the **accuracy score**, and can be computed using the following routine:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1OY39ib9YgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "print(\"Accuracy : {}\\n\".format(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "# by definition a confusion matrix C is such that C_{i, j} is equal to the number of observations \n",
        "# known to be in group i but predicted to be in group j.\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn-Wk92S9YgV",
        "colab_type": "text"
      },
      "source": [
        "Using this, we can ask how this changes as we change the model parameters, in this case the number of neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3IrNfpj9YgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vary the number of nearest neighbors\n",
        "for n_neighbors in [1, 5, 10, 20, 30]:\n",
        "    knn = KNeighborsClassifier(n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(\"# of neighbors = {}\\t Accuracy = {}\\n\".format(n_neighbors, knn.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOi9NVFPOk5C",
        "colab_type": "text"
      },
      "source": [
        "We see that in this case, a small number of neighbors seems to be the best option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8j17FpDq6HH",
        "colab_type": "text"
      },
      "source": [
        "## 4 Choosing the Correct Regularization Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv_Y9DLdM60k",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The regularization technique we have explored in *Section 2* is called \"weight decay\" or equivalently, $L2$ regularization. This heuristic prefers mild lines with small offset and slopes, to wild lines with bigger offset and slope.\n",
        "\n",
        "However, there are other regularization functions as well, and as we will discuss in the post-videos, we should decide which one is suitable for a given task:\n",
        "\n",
        "**Common Regularization Functions**:\n",
        "\n",
        "1. **L1 Regularization** (Lasso Regularization): $||\\theta||_1$\n",
        "\n",
        "2. **L2 Regularization** (Ridge Regularization): $||\\theta||_2^2$\n",
        "\n",
        "3. **Low-order Fit**: $\\sum_{q=0}^{N}(\\theta_q ^ 2 q)$ (only for polynomial models). This regularizer pays more attention to higher order weights, encouraging a lower order fit.\n",
        "\n",
        "We should have a good understanding of different regularization functions to be able to decide the best fit for your data. Lets take as an example the two most popular regularization techniques, $L1$ and $L2$.\n",
        "\n",
        "* **Computational Efficiency:** $L2$ does have easy iterative solutions, while $L1$ does not. This makes computing $L2$-norm more computationally efficient.\n",
        "\n",
        "* **Sparse Output**: $L1$-norm will squeeze most of the weights to $0$ while $L2$-norm does not. $L1$ regularization (LASSO) comes up with sparse solutions due to non-vanishing gradient at $0$. Therefore, $L1$-norm makes better sense as selecting a subset of features while eliminating the rest. $L2$-norm will make more sense when there is strong multicollinearity among the features.\n",
        "\n",
        "(Don't worry if you don't quite get this yet -- we'll explore this in the post-videos on \"Regularizing with $l^p$ Norms\")\n",
        "\n",
        "\n",
        "**BONUS**: L1 and L2 regularization are taught for pedagogical reasons. However there more sophisticated methods: a practical example is [Elastic net regularization](https://stats.stackexchange.com/questions/184029/what-is-elastic-net-regularization-and-how-does-it-solve-the-drawbacks-of-ridge/184031#184031). It's a parameterized regularization technique that is a simple balance between both L1 and L2. . The formula for Elastic net regularization is:  \n",
        "$$p * ||\\theta||_1 + (1-p) * ||\\theta||_2^2$$\n",
        "              \n",
        "A slight drawback of Elastic net regularization is that we need to find (tune; possibly through validation) a new hyperparameter $p$, to decide the weight of the $L1$ regularization component versus the $L2$ component.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP-wf32mtstx",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 5):** Suppose the number of features is much larger than the number of data points you have (common in some fields such as Computational Biology), which regularization scheme should you choose?\n",
        "\n",
        "_Choose from: $L1$ regularization, $L2$ regularization, Both will work, You should use different methods_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGPeT8lGuBgL",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 6):** Provide reasons for your answer for Question $5$.\n",
        "\n",
        "_Choose from: My chosen method will help generate more data points for better training, My chosen method will help to eliminate unnecessary data points, My chosen method will lead to feature selection_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKVTeTzZunE5",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 7):**  Please explain in your own words why $L1$ regularization leads to sparser models. (Try this before watching the post lecture videos if possible).\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cHbdt2yu8ak",
        "colab_type": "text"
      },
      "source": [
        "### .a Programming : Regularization Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ1GMtfKGW-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We start by loading some standard libraries and defining our training and validation data\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We'll again use the digit dataset to demonstrate the effect of choosing \\lambda\n",
        "dataset = datasets.load_digits() \n",
        "\n",
        "# We split our data into training and validation, the split here is arbitray\n",
        "x_train = dataset.data[:450]\n",
        "y_train = dataset.target[:450]\n",
        "x_val = dataset.data[450:]\n",
        "y_val = dataset.target[450:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gmybiYmTLnq",
        "colab_type": "text"
      },
      "source": [
        "The following code block demonstrates how you can fit a linear model using LASSO from sklearn. (The parameter we call $\\lambda$ is here called $\\alpha$, these variations in notation are unfortunately very common in ML.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_XoKicIJ-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = Lasso(alpha=0.1) # \\alpha here is our \\lambda; in sklearn that is the variable name used.\n",
        "clf.fit(x_train,y_train)\n",
        "print(\"Training score: {} Validation score: {}\".format(clf.score(x_train,y_train),clf.score(x_val,y_val)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnDytwLqUhtD",
        "colab_type": "text"
      },
      "source": [
        "**Your turn:** Just using a random value for $\\lambda$ doesn't make much sense, can you change the codeblock below  and actually determine the best value for $\\lambda$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-KVPefFJ4il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List some different \\alpha (\\lambda) values to try as part of an array \n",
        "alphas = [0.1, 0.9]\n",
        "# Store all the scores in this array\n",
        "scores_val = np.zeros(len(alphas))\n",
        "\n",
        "# Train classifier for different alpha values\n",
        "for i in range(0, len(alphas)):\n",
        "    clf = Lasso(alpha=alphas[i])\n",
        "    clf.fit(x_train, y_train)\n",
        "    scores_val[i] = clf.score(x_val,y_val)\n",
        "    \n",
        "# Print scores for different alpha\n",
        "print(scores_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoSgtZqXVOxv",
        "colab_type": "text"
      },
      "source": [
        "You might want to look at your results, feel free to use the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwzDK1tqKxVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(alphas,scores_val,label=\"Validation score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANKjXMVOD7Yj",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 8):** What value for lambda turned out to be the best (at least for this validation set)? Just type in the number (e.g., 1.00).\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCR7qMMEDjE",
        "colab_type": "text"
      },
      "source": [
        "## 5 Programming : Implement a Ridge Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW-yv3pGv0xX",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will implement a _Ridge Regression Model_, which is actually a Linear Regression model with $L2$ Regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7sLwIII_5j",
        "colab_type": "text"
      },
      "source": [
        "Let recall what is the loss function for linear regression with L2 Regularization:\n",
        "$ Loss = (Y - X * \\theta)^\\top  (Y - X * \\theta) + \\alpha * |\\theta| ^ 2$\n",
        "\n",
        "Then, the derivation of the loss with respect to the weights ( in this case $\\theta$) is:\n",
        "$ -2 X^\\top (Y - X * \\theta) + 2 * \\alpha * \\theta $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeVHuGC7FZjC",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 9):** Complete the code block to calculate **gradient descent**.\n",
        "\n",
        "_Copy the code after you add or modify_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXYZSuijDRF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradientDesc(weights,X,y,train_len,alpha,learning_rate,n_iter):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        weights : shape (n,1)\n",
        "        X : shape (m, n)\n",
        "        y : shape (m, 1)\n",
        "        train_len : # of training samples\n",
        "        alpha : regularization parameter\n",
        "        learning_rate : learning rate of the weights\n",
        "        n_iter = # of iteration\n",
        "    Returns:\n",
        "        weights : the updated weights vector\n",
        "    \"\"\"\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "        ######### Your Turn  #########\n",
        "        #\n",
        "        # Write your own code here\n",
        "        #\n",
        "        ## Update the weights here\n",
        "        #\n",
        "        ##############################\n",
        "\n",
        "    return weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aro2iX5eF0p-",
        "colab_type": "text"
      },
      "source": [
        "As you have completed the function above, let's try it to calculate _mean absolute percentage_ error of our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8wbuA3sDSsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "import random\n",
        "\n",
        "alpha = 1\n",
        "learning_rate = 0.5\n",
        "n_iter = 1500\n",
        "  \n",
        "diabetes = datasets.load_diabetes()\n",
        "diabetes_X_raw = diabetes.data\n",
        "print(\"NUMBER OF FEATURES: \", diabetes_X_raw.shape[1])\n",
        "\n",
        "# Concatenate 1 to each sample to match with the bias term\n",
        "diabetes_X = np.array(list(map(lambda x: np.concatenate(([1], x)), diabetes_X_raw)))\n",
        "\n",
        "# Use 100 samples for test set.\n",
        "train_x = diabetes_X[:-100]\n",
        "test_x = diabetes_X[-100:]\n",
        "train_y = diabetes.target[:-100]\n",
        "test_y = diabetes.target[-100:]\n",
        "    \n",
        "train_len = train_x.shape[0]\n",
        "weights = [0.0] * (train_x.shape[1]) # Weights or parameter vector initialized with 0\n",
        "weights = np.array(weights)\n",
        "\n",
        "# To train on train data set\n",
        "weights = gradientDesc(weights, train_x, train_y,train_len, alpha, learning_rate, n_iter)\n",
        "    \n",
        "# Even though we optimized the weights on MSE with regularization error, we use Mean Absolute Percentage Error as test metric, \n",
        "residuals = np.sum(np.array(test_x) * weights, axis=1) - test_y\n",
        "residuals = [(abs(a)/b)*100 for a,b in zip(residuals,test_y)]\n",
        "cost = sum(residuals)/len(residuals)\n",
        "\n",
        "print ('Mean Absolute Percentage Error of our Model on test set:',cost,'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9Qbqp7KbJa",
        "colab_type": "text"
      },
      "source": [
        "Then, we load the Ridge function from sklearn library. If the Mean Absolute Percentage Error of this library is comparable to your implemented function, then it is likely to be correct!  **Note:** Notation for Mean Absolute Percentage Error = $\\frac{1}{m} \\sum_{i=1}^m \n",
        "\\frac{|y_{predict}^i - y_{true}^i|} {|y_{true}^i|}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyxcIKhXJ1nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "clf = Ridge(alpha= alpha, solver='sag', max_iter=1500) #We use the stochastic averaged gradient descent algorihtm since batch GD isn't available\n",
        "\n",
        "clf.fit(train_x, train_y)\n",
        "\n",
        "pred = clf.predict(test_x)\n",
        "residuals = pred - test_y\n",
        "residuals = [(abs(a)/b)*100 for a,b in zip(residuals,test_y)]\n",
        "cost = sum(residuals)/len(residuals)\n",
        "print ('Mean Absolute Percentage Error of built-in model on test set:',cost,'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3CM2Zb2--c2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Actually there is a closed-form solution for the Ridge Regression, and it is easier to derive compared to gradient descent approach. So, why do we even attempt the gradient descent approach here? Hmmmmm...\n",
        "\n",
        "Actually, closed-form solutions for other cases are often infeasible to find in general, so a generalized solution is needed.  As mentioned in lecture in Week 4, this is  why we often perform gradient descent.  Here, we ask you to try it to be familiar with its workings. :D\n",
        "\n",
        "How about cases where gradient descent is not possible, because the loss function is not differentiable? There are other methods, such as subgradient methods but we will leave it for your own exploration.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLiJhwUIBYfD",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "# Week 06: Post-tutorial Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7phNXATESWP",
        "colab_type": "text"
      },
      "source": [
        "Watch the Week 06 post-videos on the lecture topics introduced this week, then attempt the following exercises.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EJF-oc6BYfW",
        "colab_type": "text"
      },
      "source": [
        "## 6 Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ekIF5ue4KUT",
        "colab_type": "text"
      },
      "source": [
        "### .a Check your Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofzqHIVoJxDz",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 1):** Describe how you would divide a dataset of time series nature (e.g stock prices) for validation purposes?\n",
        "\n",
        "_Choose from: Just use `sklearn`'s train-test split function, Randomly choose using `numpy`, Divide into consecutive blocks of time frame_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVtM4XD2KJpJ",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 2):** Alex built a computer vision model that can detect people eating or texting while driving. To train the model, he has a bunch of pictures of people activities while driving, the dataset comes from $1,000$ different men. Then the dataset is randomly divided into validation and training set, which gives him a very good model. Do you think that this model will be suitable for real-life testing? (Hint: Are we doing validation correctly?)\n",
        "\n",
        "_Choose from: Yes, No_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP9jOgKjKQa-",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 3):** Give reasons for your previous answer.\n",
        "\n",
        "_Choose from: Dividing randomly is a good idea to generalize, He should not have divided randomly as this will lead to information leakage, There are no women in the dataset, He should have more people in his dataset_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8hKB-O0TFT6",
        "colab_type": "text"
      },
      "source": [
        "### .b Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XJ09oHTTJx0",
        "colab_type": "text"
      },
      "source": [
        "Let $K$ be the size of validation set: \n",
        "\n",
        "*   $K \\uparrow$ => $J_{val} \\approx J_{test}$\n",
        "*   $K \\downarrow$ =>  $N - K \\uparrow$ => More data for training set => Lower $J_{train}, J_{val}$ and $J_{train} \\approx J_{val}$\n",
        "\n",
        "So, can we have both big and small $K$ at the same time??? Actually the answer is \"sort of\". We can create a validation set of size $K$ where $K$ is big, however we can reuse that validation set for training. The procedure is called **K-fold Cross Validation** as described below:\n",
        "\n",
        "\n",
        "1.   Split the dataset $\\mathcal{D}$ into $K$ separate sets of equal size.\n",
        "  *   Suppose $\\mathcal{D} = (\\mathcal{D}_1, \\mathcal{D}_2, ... ,\\mathcal{D}_K)$\n",
        "  *   Commonly chosen $K$ are $K$ = 5 and $K$ = 10\n",
        "2.   For each $k$ = 1,2,...,$K$, we fit the model to training set **excluding** the $k$th-fold $T_k$\n",
        " > 2.1 Compute the fitted values for the observations in $\\mathcal{D}_k$, based on the training data that excluded this fold.\n",
        "               \n",
        " > 2.2 Compute the cross-validation (CV) error for the $k$-th fold:  $(CV Error)_k^\\lambda$\n",
        "3.  The model then has overall cross-validation error: $(CV Error)^\\lambda= \\frac{\\sum_{k=1}^{K} (CV Error)_k}{K}$\n",
        "4.   We will then proceed to choose $\\lambda$ such that $(CV Error)^\\lambda$ is minimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMp78tFeTOAq",
        "colab_type": "text"
      },
      "source": [
        "Let's get back to the example code in the pre-tutorial section. In the above, we've only used $3/4$ of the data for the training, and used $1/4$ for the validation. Another option is to use **2-fold cross-validation**, where we split the sample in half and perform the validation for each half separately. When we train the classifier using the first half ($X1$), we test it using the other half ($X2$), and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoSWJMlkVlz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Digits dataset\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Split into train and test data\n",
        "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state= 11)\n",
        "print(\"Size of X1 = {}; Size of X2 = {}\".format(len(X1),len(X2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIRJQWjvVnep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors = 1)\n",
        "knn.fit(X1, y1)\n",
        "print(\"Accuracy (training on first half) : {}\".format(knn.score(X2, y2)))\n",
        "knn.fit(X2, y2)\n",
        "print(\"Accuracy (training on second half) : {}\".format(knn.score(X1, y1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLTG2nmVVrAh",
        "colab_type": "text"
      },
      "source": [
        "Thus, two-fold cross-validation gives us two estimates of the score for that parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB52jnMuVwlY",
        "colab_type": "text"
      },
      "source": [
        "### .c K-fold Cross-Validation\n",
        "\n",
        "Here we've used 2-fold cross-validation. This is just one specialization of $K$-fold cross-validation, where we split the data into $K$ chunks and perform $K$ fits, where each chunk gets a turn being the validation set. \n",
        "\n",
        "There is an easy to do $K$-fold cross validation using ```scikit-learn```. We can do this by changing the ``cv`` parameter shown below. Let's do 10-fold cross-validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dDFmv6AV2Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# setting cv parameter to 10, for 10-fold cross validation\n",
        "cv = cross_val_score(KNeighborsClassifier(1), X, y, cv=10)\n",
        "print(\"Accuracy of different fold\\n {}\\n\".format(cv))\n",
        "print(\"The mean accuracy of all folds: {}\".format(cv.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZAAvarIV0XL",
        "colab_type": "text"
      },
      "source": [
        "This gives us an even better idea of how well our model is doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yxLBCXYH-sD",
        "colab_type": "text"
      },
      "source": [
        "**Remarks:**\n",
        "\n",
        "1. Our data set might be small, so we might not have enough observations to put aside a test set. In this case:\n",
        "    > 1.1.    Let all of the available data be our training set. \n",
        "    \n",
        "    > 1.2.    Still apply $K$-fold cross validation. Then we still choose $\\alpha^*$ as the minimizer of CV error\n",
        "    \n",
        "    > 1.3.   Then refit the model with $\\alpha^*$ on the entire training set\n",
        "\n",
        "2. In terms of computational efficiency, $K$-fold cross validation is quite inefficient because the algorithm has be to performed repeatedly for $K$ times. In the extreme case where $ K \\equiv m$, we have a special name which is **Leave One Out Cross Validation**. Actually, there is a much more efficient algorithm for Leave One Out Cross Validation rather than performing the algorithm repeatedly for $m$ times, but we will leave them for your own exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24dtzQmvJELx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### .d The wrong and right way to validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-760zl6iBYfO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Consider a classification problem with a large number of predictors, as may\n",
        "arise, for example, in genomic or proteomic applications. A typical strategy\n",
        "for analysis might be as follows:\n",
        "\n",
        "1. Screen the predictors: find a subset of “good” predictors that show\n",
        "fairly strong (univariate) correlation with the class labels\n",
        "2. Using just this subset of predictors, build a multivariate classifier.\n",
        "3. Use cross-validation to estimate the unknown tuning parameters and\n",
        "to estimate the prediction error of the final model.\n",
        "\n",
        "Is this a correct application of cross-validation? Consider a scenario with\n",
        "$m$ = 50 samples in two equal-sized classes, and $p$ = 5000 quantitative\n",
        "predictors (standard Gaussian) that are independent of the class labels.\n",
        "The true (test) cost of any classifier is 50%. We carried out the above\n",
        "recipe, choosing in step (1) the 100 predictors having highest correlation\n",
        "with the class labels, and then using a 1-nearest neighbor classifier, based\n",
        "on just these 100 predictors, in step (2). Over 50 simulations from this\n",
        "setting, the average CV cost was 3%. This is far lower than the true\n",
        "cost of 50%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNBXXHdnJm0o",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 4):** What do you think happened there in the above example? If possible, please suggest a modification of the procedure.\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WBjlvLOOSB7",
        "colab_type": "text"
      },
      "source": [
        "## 7 Check Your Understanding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g_bh3yIMuIM"
      },
      "source": [
        "**1.** Here you will need to try to derive the closed-form formula for ridge regression, using the derivative of the loss function we gave in the pre-tutorial notebook. \n",
        "\n",
        "**Hint**: What is the condition of the derivative of the loss function for local minima/maxima?  \n",
        "\n",
        "**Your Turn (Question 5):** What is the closed-form solution for *Ridge Regression*?\n",
        "\n",
        "_Choose one from the following options:_\n",
        "> **A** : $(X +\\lambda I)^{-1}X^\\top Y$\n",
        "\n",
        "> **B** : $(X^\\top Y +\\lambda I)^{-1}X^\\top X$\n",
        "\n",
        "> **C** : $(X^\\top Y)^{-1}(X^\\top X +\\lambda I)$\n",
        "\n",
        "> **D** : $(X^\\top X +\\lambda I)^{-1}X^\\top Y$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HUh5vAu66_j",
        "colab_type": "text"
      },
      "source": [
        "**2.** In the previous section, we are told that the reason why we use gradient descent for ridge regression is because we cannot always find a closed-form solution for a regularized problem. However that is not the only reason. Actually, calculating the closed-form solution for ridge regression is also more computationally expensive compared to the gradient descent approach (Look at the formula you derive above, the inverse of the big matrix is the key issue). \n",
        "\n",
        "But in the pre-tutorial notebook code example, when we use batch gradient descent it is actually slower than the closed-form approach. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1qnVbwnNde8",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 6):** Can you suggest how to improve it?\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41j2o-udfqL6",
        "colab_type": "text"
      },
      "source": [
        "You have learned about correct way to do validation for time-series data. How about cross- validation?\n",
        "\n",
        "**Your Turn (Question 7):** Suppose you are given a time-series of  $[1, 2, 3, 4  ]$, and you need to do leave-one-out cross validation. Among these two options, which one is correct:\n",
        "\n",
        "> **A: (Train[$1$], Test[$2$]), (Train[$1,2$], Test[$3$]), (Train[$1,2,3$], Test[$4$])**\n",
        "\n",
        "> **B: (Train[$2,3,4$], Test[$1$]), (Train[$1,3,4$], Test[$2$]), (Train[$1,2,4$], Test[$3$]), (Train[$2,3,4$], Test[$1$])**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubJInlUXGwKk",
        "colab_type": "text"
      },
      "source": [
        "## 8 Total Variation Regularization for Image Denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSw2T9SFEv0R",
        "colab_type": "text"
      },
      "source": [
        "In this section we will learn an application of regularization in image denoising. Simple approach, but good results!\n",
        "\n",
        "We consider 2D signals (i.e images), where $y_{i, j}$ is defined as the pixel value at location (i, j). The total variation norm of the image is defined as:\n",
        "\n",
        "$Variation(y) = \\sum_{i, j} |y_{i+1, j} - y_{i, j}| + |y_{i, j+1} - y_{i, j}|$\n",
        "\n",
        "Our goal is to transform the image to a new image where the new image is as close to the original image as possible but the total variation should also be minimized.\n",
        "\n",
        "To measure the closeness of the two images, we can make use of this metric:\n",
        "\n",
        "$J(x, y) = \\frac{1}{2} \\sum_{i, j} |x_{i. j} - y_{i, j}|$\n",
        "\n",
        "If we think of the total variation as something we need to give regularization, then our goal is to find a new image $x$ where:\n",
        "\n",
        "$ J(x, y) + \\lambda Variation(x)$\n",
        "\n",
        "Looks familiar? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJNBLsCkC1E",
        "colab_type": "text"
      },
      "source": [
        "### .a Code Example\n",
        "Let's play with total variation image denoising to see how effective it is. Note that this approach is much better than simple denoise method such as median filter because it is proved to retain the edge information better.\n",
        "\n",
        "_Note: This code does take awhile to run- please be patient!_\n",
        "\n",
        "Reference: This code is taken from http://scikit-image.org/docs/0.6/auto_examples/plot_lena_tv_denoise.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr2zKhFPoc0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install 'scikit-image<0.12.0'\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    from skimage.restoration import denoise_tv_chambolle\n",
        "except ImportError:\n",
        "    # skimage < 0.12\n",
        "    from skimage.filters import denoise_tv_chambolle\n",
        "\n",
        "l = scipy.misc.ascent()\n",
        "l = l[230:290, 220:320]\n",
        "\n",
        "# Play around with this k value\n",
        "k = 0.4\n",
        "noisy = l + k*l.std()*np.random.random(l.shape)\n",
        "\n",
        "# Play around with this weight value. The higher the weight, the larger the regularization\n",
        "# cost => The stronger the denoising is (but with the expense of losing fidelity)\n",
        "\n",
        "tv_denoised = denoise_tv_chambolle(noisy, weight=10)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 2.8))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.imshow(noisy, cmap=plt.cm.gray, vmin=40, vmax=220)\n",
        "plt.axis('off')\n",
        "plt.title('noisy', fontsize=20)\n",
        "plt.subplot(132)\n",
        "plt.imshow(tv_denoised, cmap=plt.cm.gray, vmin=40, vmax=220)\n",
        "plt.axis('off')\n",
        "plt.title('TV denoising', fontsize=20)\n",
        "\n",
        "tv_denoised = denoise_tv_chambolle(noisy, weight=50)\n",
        "plt.subplot(133)\n",
        "plt.imshow(tv_denoised, cmap=plt.cm.gray, vmin=40, vmax=220)\n",
        "plt.axis('off')\n",
        "plt.title('(more) TV denoising', fontsize=20)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.02, hspace=0.02, top=0.9, bottom=0, left=0,\n",
        "                    right=1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdaEmCwLNoMi",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 8):** For $k = 0.6$, play around with the weights (think of weight as similar to lambda in regularization). Give the approximation of the best weight (or $\\lambda$).\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kGaO2wXmyVP",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Credits\n",
        "Authored by Mohammad Neamul Kabir, Poh Jie, Le Trung Hieu, Martin Strobel and [Min-Yen Kan](http://www.comp.nus.edu.sg/~kanmy) (2019), affiliated with [WING](http://wing.comp.nus.edu.sg), [NUS School of Computing](http://www.comp.nus.edu.sg) and [ALSET](http://www.nus.edu.sg/alset).\n",
        "Licensed as: [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/ ) (CC BY 4.0).\n",
        "Please retain and add to this credits cell if using this material as a whole or in part.   Credits for photos given in their captions."
      ]
    }
  ]
}