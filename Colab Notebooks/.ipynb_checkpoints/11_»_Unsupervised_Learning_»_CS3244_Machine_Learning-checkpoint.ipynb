{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11 » Unsupervised Learning » CS3244 Machine Learning","provenance":[{"file_id":"1G49PU5V1PsnFPB9Sp8SJu8x56tVRPdBD","timestamp":1572297188958},{"file_id":"1dTUMuZjC8QY7MaVZIowJZbEDZ58OodIT","timestamp":1572191320498},{"file_id":"1hY03sjX6emZqSZjOg5hh-vaGG0nHgESY","timestamp":1571713998653},{"file_id":"1LJdfh_HhCRGJOQb4ocpsaQHcc38VloiM","timestamp":1566305751142},{"file_id":"18cNQ_F_b3bRtfQhZ6I9OxTrE3DACJrDZ","timestamp":1541567420670},{"file_id":"108C6NR338DIKGZFgP6o93RiH69C7rzo8","timestamp":1541563830656},{"file_id":"1dDq7fCxGozOT_8mhg3CJgjtHEk5gjhEh","timestamp":1540876224068},{"file_id":"1-iJBjhsIBDUD_752j-FO9bFEO9Tcz4vx","timestamp":1532270405693},{"file_id":"10T1tSBCwsc1lOp7XXV2ED3P0nAppr2Tq","timestamp":1532270150764}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OGbGtHWvN33e","colab_type":"text"},"source":["Available at http://www.comp.nus.edu.sg/~cs3244/1910/11.colab"]},{"cell_type":"markdown","metadata":{"id":"uvf81x6Vrysn","colab_type":"text"},"source":["![Machine Learning](https://www.comp.nus.edu.sg/~cs3244/1910/img/banner-1910.png)\n","---\n","See **Credits** below for acknowledgements and rights.  For NUS class credit, you'll need to do the corresponding _Assessment_ in [CS3244 in Coursemology](http://coursemology.org/courses/1677) by the respective deadline (as in Coursemology). \n","\n","**You must acknowledge that your submitted Assessment is your independent work, see questions in the Assessment at the end.**"]},{"cell_type":"markdown","metadata":{"id":"I-NrlpANkdjJ","colab_type":"text"},"source":["**Learning Outcomes for this Week** \n","\n","After watching the videos and completing the exercises for this week, you should be able to:\n","\n","* Have a basic understanding of unsupervised learning;\n","* Understand different unsupervised learning algorithms for:\n","  * Dimensionality reduction: For aiding visualization, exploratory analysis;  \n","  * Clustering: Finding latent patterns.\n","* Implement and understand $k$ means clustering;\n","* Understand the Expectation Maximization meta-algorithm;\n","  * and how estimating the Gaussian Mixture Model exemplifies it;\n","* Apply unsupervised learning on real-world data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LlWrMKNcdlDG","colab_type":"text"},"source":["*Welcome to the Week $11$ Python notebook.* This week we will learn about **Unsupervised Learning**. We introduce the topic in the lecture videos, and will be reviewing this material in the ninth tutorial.\n","\n","In the notebook, we go through popular algorithms in unsupervised learning; namly *PCA*, *$k$-means*, *Gaussian Mixture Model*. In the pre-tutorial notebook, we look into an example of PCA and implement $k$-means from scratch. We look through modification of $k$-means algorithm in the post-tutorial portion. Also, we look into a few problems of using $k$-means and try to overcome them using Gaussian Mixture Model."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GVHBrmnbGVXz"},"source":["---\n","# Week 11: Pre-Tutorial Work\n","\n","Watch the videos for Week $11$ Pre and answer the following question."]},{"cell_type":"markdown","metadata":{"id":"ExZw__rWeuUA","colab_type":"text"},"source":["**Your Turn (Question 1):** Which of the following is not an example of unsupervised learning?\n","\n","_Choose from: Social network analysis to define group of friends, Organizing computing clusters based on similar event patterns and processes, Answering multiple choice questions_"]},{"cell_type":"markdown","metadata":{"id":"e0UFWV_lsjxz","colab_type":"text"},"source":["## 1 Programming : Face Recognition using PCA"]},{"cell_type":"markdown","metadata":{"id":"dbjAq2MKD1SF","colab_type":"text"},"source":["In the pre-videos, we learned about **PCA** and **Eigenfaces**, so now we will implement a **face recognition** algorithm using these methods with `scikit-learn`. For this example, we will use the image dataset [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/lfw.pdf) (LFW), which contains images with labels. We will use a percentage of these images to train a **Logistic Regression** classifier and try to predict unseen images. Before doing that we will use *PCA* to reduce the dimensionality of the images, and then use those for our model. \n","\n","Ready?  Let's get started on our implementation!"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zXEVz0IPGVYQ"},"source":["### .a Setup\n","The usual formalities.  Let's get our `import`s done to get our toolkit online."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TzADWw_3GVYS","colab":{}},"source":["# import necessary libraries\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.datasets import fetch_lfw_people\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_fVUjs2D5eY","colab_type":"text"},"source":["### .b Load Dataset\n","We will now download our `lfw_people` dataset using `sklearn`. Note that, in the dataset, we have different number of images for each person, some may have $70$ images, some $10$ and so on. We are loading only those people's images who have at least **$60$** images in the dataset."]},{"cell_type":"code","metadata":{"id":"XaGJul70DvFK","colab_type":"code","colab":{}},"source":["# Download the data and load it in as a set of numpy arrays\n","# min_faces_per_person limits the dataset to have the image with at least that amount\n","lfw_people = fetch_lfw_people(min_faces_per_person=60, resize=0.4)\n","\n","# introspect the images arrays to find the shapes (for plotting)\n","n_samples, h, w = lfw_people.images.shape\n","\n","X = lfw_people.data\n","n_features = X.shape[1]\n","\n","# the label to predict is the ID of the person\n","y = lfw_people.target\n","target_names = lfw_people.target_names\n","n_classes = target_names.shape[0]\n","\n","print (\"\\nTotal dataset size:\")\n","print (\"n_samples: %d\" % n_samples)\n","print (\"n_features: %d\" % n_features)\n","print (\"n_classes: %d\" % n_classes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_t5g_Ga00Hvb","colab_type":"text"},"source":["Now that we have loaded the dataset, let's ensure that we know what you're working with (Rule 1: know thy dataset).  Let's examine several faces! Put an integer from $1$ to `n_samples` in the `visualize_face` variable and see whose image it is!"]},{"cell_type":"code","metadata":{"id":"d9xiGnjxfD5i","colab_type":"code","colab":{}},"source":["######### Put in an integer from 1 to n_samples ###########\n","visualize_face = 40\n","\n","# Plotting the grey image of the above index\n","plt.figure(figsize=(3.6, 4.8))\n","plt.imshow(X[visualize_face].reshape((h, w)), cmap=plt.cm.gray)\n","plt.xticks(())\n","plt.yticks(())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uGerUxvxlDRm","colab_type":"text"},"source":["We will split our dataset into training and testing using `sklearn`."]},{"cell_type":"code","metadata":{"id":"nVaBJmWKlCo9","colab_type":"code","colab":{}},"source":["# split into a training (75%) and testing (25%) set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2-rolvXH4TP","colab_type":"text"},"source":["### .c Applying PCA\n","The next step is dimensionality reduction. From the above output we have seen that there are in total $1850$ features in the dataset, which is a lot (!). We will use scikit-learn's  `PCA` class to perform the dimensionality reduction. For that, we have to select the **number of components**, i.e. the output dimensionality (number of *eigenvectors* to project onto). We can tune this parameter to get a better result. For starters, let's use $150$ components."]},{"cell_type":"code","metadata":{"id":"to5OLCt2H3pa","colab_type":"code","colab":{}},"source":["######### You may change this parameter later ###########\n","# Number of components for PCA\n","n_components = 150\n","########################################################\n","\n","print(\"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0]))\n","\n","pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True, random_state=42).fit(X_train)\n","\n","# reconstructing the images from pca output --> eigenfaces\n","eigenfaces = pca.components_.reshape((n_components, h, w))\n","\n","# Applying pca to training and test set\n","X_train_pca = pca.transform(X_train)\n","X_test_pca = pca.transform(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3tg8p6bEBWk","colab_type":"text"},"source":["### .d Training Linear Classifier on the PCA results\n","Now that we have reduced the number of dimensions, let's use the transformed data to train our SVM classifier.  \n","\n","We'll employ a Logistic Regression with Limited-memory BFGS (`lbfgs`) as optimizer and multinomial loss. You can read more about the different parameters of Logistic Regression from [`sklearn` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n","\n","\n","OK. Let's train our classifier!\n"]},{"cell_type":"code","metadata":{"id":"ymiKBK4ng8CQ","colab_type":"code","colab":{}},"source":["print(\"Fitting the classifier to the training set\")\n","\n","# creating Logistic Regression classifier instance\n","clf = LogisticRegression(random_state=11, solver='lbfgs', multi_class='multinomial', max_iter=1000)\n","\n","# Training our classfier\n","clf = clf.fit(X_train_pca, y_train)\n","\n","# Quantitative evaluation of the model quality on the test set\n","print(\"Predicting people's names on the test set\\n\")\n","# Predictin for the test set\n","y_pred = clf.predict(X_test_pca)\n","\n","# Prediction results\n","print(classification_report(y_test, y_pred, target_names=target_names))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WfG-49hlnuS0","colab_type":"text"},"source":["### .e Plotting the prediction\n","Here are a few functions so that, we can visualize the images with true label and predicted label."]},{"cell_type":"code","metadata":{"id":"EinrNR8IDoRY","colab_type":"code","colab":{}},"source":["def plot_gallery(images, titles, h, w, n_row=3, n_col=5):\n","    \"\"\"Helper function to plot a gallery of portraits\n","    \n","       inputs:\n","          images (numpy array) : array of images that we want to show\n","          titles (string array) : \n","          h (int) : height of the image\n","          w (int) : width of the image\n","          n_row (int) :\n","          n_col (int) :\n","    \"\"\"\n","    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n","    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n","    for i in range(n_row * n_col):\n","        plt.subplot(n_row, n_col, i + 1)\n","        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n","        plt.title(titles[i], size=12)\n","        plt.xticks(())\n","        plt.yticks(())\n","        \n","\n","def title(y_pred, y_test, target_names, i):\n","    \"\"\"Helper function to generate the title\n","    inputs:\n","      y_pred : List of predictions \n","      y_test : \n","    \n","    outputs:\n","    \"\"\"\n","    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n","    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n","    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gYqVjlEriqc","colab_type":"text"},"source":["Let's visualize a few images from the dataset to see the prediction results."]},{"cell_type":"code","metadata":{"id":"gAZtX-pnDlrI","colab_type":"code","colab":{}},"source":["# Generate titles with predicted and true labels for the images\n","prediction_titles = [title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])]\n","# Call the plot function with necessary parameters\n","plot_gallery(X_test, prediction_titles, h, w)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lk9ePlFlsXFC","colab_type":"text"},"source":["**Optional:** We already have seen the prediction output. One last thing we can do, is visualize the **eigenfaces**, reconstructed from the `PCA` output. Let's look at a few images, how they look like!"]},{"cell_type":"code","metadata":{"id":"N6Fx-p9cKVgn","colab_type":"code","colab":{}},"source":["# plot the gallery of the most significative eigenfaces\n","eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n","\n","plot_gallery(eigenfaces, eigenface_titles, h, w)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t792QZX-sz8f","colab_type":"text"},"source":["**Your Turn:** We have used `n_components = 150` for `PCA`. Now you need to tweak the parameter with different values and select the best one. You may consider looking at the average *precision, recall and f1-score* from the classification report.."]},{"cell_type":"markdown","metadata":{"id":"QIeaqGkoxUwN","colab_type":"text"},"source":["**Your Turn (Question 2):** Which of the following values for `n_components` in PCA gives you better classification result?\n","\n","_Choose from: 50, 150, 300_"]},{"cell_type":"markdown","metadata":{"id":"Z4Cv-WEqykok","colab_type":"text"},"source":["**Your Turn (Question 3):** Why you get good result for stated `n_components` value in the previous question? Explain your answer.\n","\n","_Replace with your answer_"]},{"cell_type":"markdown","metadata":{"id":"LNke9mGtskSb","colab_type":"text"},"source":["## 2 Programming : $k$-means Clustering\n","\n","\n","In this exercise, we will implement basic $k$-means clustering algorithm from scratch. We will see limitations of the $k$-means algorithm and try to improve on them."]},{"cell_type":"markdown","metadata":{"id":"UmV0Zd9djipe","colab_type":"text"},"source":["### .a Setup\n","At the beginning of our implementation we will first `import` necessary libraries."]},{"cell_type":"code","metadata":{"id":"6jqwJ-8C66iy","colab_type":"code","colab":{}},"source":["# import necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","\n","import seaborn as sns; sns.set()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QXFigxrnBmG","colab_type":"text"},"source":["We will generate data points for our exercise using the `sklearn` library. For this exercise, we will be using $4$ clusters as our input data. "]},{"cell_type":"code","metadata":{"id":"yhzwhmEZkXGL","colab_type":"code","colab":{}},"source":["from sklearn.datasets.samples_generator import make_blobs\n","\n","# Generating 300 data points with 4 centers\n","X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n","\n","# Plot the generated data points\n","plt.scatter(X[:, 0], X[:, 1], s=50);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UrQCPVgW5Cb3","colab_type":"text"},"source":["### .b $k$-means from `sklearn`"]},{"cell_type":"markdown","metadata":{"id":"Sk_h0ryjnRmQ","colab_type":"text"},"source":["Before implementing the $k$-means algorithm from scratch, let's have a look at the `sklearn` version."]},{"cell_type":"code","metadata":{"id":"utXhVaaykkCZ","colab_type":"code","colab":{}},"source":["# Import KMeans from sklearn\n","from sklearn.cluster import KMeans\n","\n","# Creating an instance of K-means for 4 clusters\n","kmeans = KMeans(n_clusters=4)\n","kmeans.fit(X)\n","\n","# Get the prediction for the data points\n","y_kmeans = kmeans.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjc9nfgmn1ES","colab_type":"text"},"source":["We have the prediction for our data points in `y_kmean`. Let's visualize the predicted clusters."]},{"cell_type":"code","metadata":{"id":"daBAZT2IknQ5","colab_type":"code","colab":{}},"source":["# Plot the output of sklearn version of k-means\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n","\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Q5s2zw4qzqP","colab_type":"text"},"source":["### .c Implementing $k$-means from scratch"]},{"cell_type":"markdown","metadata":{"id":"LNpAbyJCppqS","colab_type":"text"},"source":["We will start implementing $k$-means by ourselves. In this version of $k$-means, we will start with random **centers** for our clusters. \n","\n","The function `randomCenters` generates initial random centers for our $k$-means algorithm."]},{"cell_type":"code","metadata":{"id":"aSvb3XrvL06g","colab_type":"code","colab":{}},"source":["import random\n","\n","def randomCenters(X, n_clusters, rand_seed):\n","  \"\"\"\n","    Args:\n","      X (m x 2) : input 2D data points\n","      n_clusters (int) : number of clusters\n","      rand_seed (int) : seed for random number generation\n","    \n","    Returns:\n","      centers (array of floats) : random centers\n","  \"\"\"\n","  # Size of our data set\n","  sz = len(X)\n","  \n","  # Use the random seed to get same output for everyone\n","  random.seed(rand_seed)\n","  \n","  # Generate four (n_clusters) random positions\n","  positions = random.sample(range(1,sz), n_clusters)\n","  \n","  # Assign those points as initial centers\n","  centers = X[positions]\n","  \n","  return centers"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gI__Sbpir-35","colab_type":"text"},"source":["  `plotCenter` is another helper function to plot the centers and the cluster assignments."]},{"cell_type":"code","metadata":{"id":"SIYaltiD9GQA","colab_type":"code","colab":{}},"source":["def plotCenter(X , centers, labels, titleNumber,  titleStr):\n","  \"\"\"\n","    Args:\n","      centers (array of float) : array of centers\n","      labels (array of integer) : cluster assignment of data points\n","      titleNumber (int) : Random seed for current iteartion or iteration number to show in the title\n","      titleStr (string) : Plot title\n","      \n","  \"\"\"\n","  plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');\n","  plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);\n","  plt.suptitle(\"%s %d\" % (titleStr,titleNumber))\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"srQNx1jpta5r","colab_type":"text"},"source":["The function `pairwiseDistance` calculates the distance between two given data points. We will use this function to calculate the nearest cluster and assign the point to the nearest cluster."]},{"cell_type":"code","metadata":{"id":"qhX5Zzi5tZcF","colab_type":"code","colab":{}},"source":["def pairwiseDistance(x1, x2):\n","  \"\"\"\n","    Args:\n","      x1 (2D point) : one data point\n","      x1 (2D point) : another data point\n","    Returns:\n","      dist (float) : distance between the two data points\n","  \"\"\"\n","  dist = np.linalg.norm(x1-x2, axis=0)\n","  return dist"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXhH21Rits4J","colab_type":"text"},"source":["Great! Now we have everything we need. All we need to do now is to implement the $k$-means algorithm. Let's recall the $k$-means algorithm first.\n","\n","**$k-$ _means algorithm:_**\n","\n","$\n","\\textbf{Data: } \\text{a set of points, $x^{(j)} \\in \\mathbb{R}^n$}\\\\\n","\\textbf{Output: } \\text{a $m$-tuple of clusters, $(y^{(1)}, \\dots, y^{(m)})$} \\text{where each $y^{(i)} \\in \\{1,,2, \\dots, k\\}$}\\\\\n","\\textbf{for } i=1 \\text{ to } k \\text{ do:} \\\\\n","\\hspace{5mm}\\mu_i \\leftarrow \\text{ some random location} \\\\\n","\\textbf{repeat} \\text{ until converged:}\\\\\n","\\hspace{5mm} \\textbf{for } j=1 \\text{ to }m \\text{ do:} \\\\\n","\\hspace{10mm}y^{(j)} \\leftarrow \\underset{i}{\\text{ arg min}} ||\\mu_i - x^{(j)}||\\\\\n","\\hspace{5mm} \\textbf{for } i=1 \\text{ to } k \\text{ do:} \\\\\n","\\hspace{10mm} S_i \\leftarrow \\{x^{(j)}|y^{(j)} = i \\}\\\\\n","\\hspace{10mm} \\mu_i \\leftarrow \\frac{1}{|S_i|} \\underset{x^{(j)}\\in S_i}{\\sum}x^{(j)}\\\\\n","\\textbf{return }(y^{(1)}, \\dots, y^{(m)})\n","$\n"]},{"cell_type":"markdown","metadata":{"id":"QTWrFDdCw0Z0","colab_type":"text"},"source":["We will now implement the algorithm above in Python. Don't worry, you don't have to write everything. We have already written most of the code, we just need a little help to finish the code. "]},{"cell_type":"markdown","metadata":{"id":"vMCURRVjDHF0","colab_type":"text"},"source":["**Your Turn (Question 4):** Complete the code below to implement $k$-means clustering."]},{"cell_type":"code","metadata":{"id":"jby8ylEzyfu0","colab_type":"code","colab":{}},"source":["import random\n","\n","def kMeansClustering(X, n_clusters, centers, plot_steps = 0, max_iter = 100):\n","    \"\"\"\n","    Args:\n","      X (mx 2): Input data points\n","      n_clusters (int) : number of clusters\n","      centers (array of float) : initial centers\n","      plot_steps (int) : if set to 1 shows each iteration of the algorithm, deafult value 0\n","      max_iter (int) : maximum number of iteration, deafult value 100\n","    \n","    Returns:\n","      labels (m x 1) : cluster assignment of each point\n","      centers (array of float) : final centers of the clusters\n","    \"\"\"\n","    \n","    # Length of our data set\n","    sz = len(X)\n","    \n","    # Initializing labels array, where we will store the cluster assignment\n","    labels = np.zeros(sz)\n","    \n","    # Loop until max iteartion and recalculate center at each step\n","    for iterator in range(max_iter):\n","        # Check if we want to show each iteration of the algorithm\n","        if plot_steps:\n","            plotCenter(X, centers, labels, iterator, \"Iteration\")\n","        \n","        #########################################\n","        # Your Turn: write code here\n","        #\n","        # Loop through each point\n","        # For each point : calculate distance for different clusters\n","        # Assign the cluster with minimum distance in labels array\n","        #\n","        # Recalculate the centers, store in new_centers from the means of the newly assigned points \n","        #\n","        #########################################\n","        \n","        # if centers are same as previous, break the loop and return current centers and labels\n","        if np.all(centers == new_centers):\n","            break\n","          \n","        centers = new_centers\n","    \n","    return labels, centers   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZoRbKWdV1KH4","colab_type":"text"},"source":["Let's check if your implementation works!"]},{"cell_type":"code","metadata":{"id":"bon1m_370K15","colab_type":"code","colab":{}},"source":["# Assign number of clusters\n","n_clusters = 4\n","\n","# Initial centers with seed 11, so that everybody can have same answers\n","centers = randomCenters(X, n_clusters, rand_seed=11)\n","\n","# Calling our implemented function, with plot_steps set to 1 so that we can see each iteration\n","labels, new_centers = kMeansClustering(X, n_clusters, centers, plot_steps=1)\n","#print(new_centers)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ooi_Rx6Nm1kC","colab_type":"text"},"source":["### .d Run $k$-means with different random seed"]},{"cell_type":"markdown","metadata":{"id":"LZKhEkrx1zuY","colab_type":"text"},"source":["Great work! Our implementation seems to work fine. But does it always give correct result? In this implementation, we randomly assign our initial centers. Let's try with different initialization, by having different *random seed*.  In the code snippet below, we will use $5$ different seeds and visualize the final output. Let's execute the code and see the result!"]},{"cell_type":"code","metadata":{"id":"AauWcN0GVXTb","colab_type":"code","colab":{}},"source":["# Assign number of clusters\n","n_clusters = 4\n","\n","# Five different seeds\n","seeds = [19, 21, 27, 29, 85]\n","\n","# Run our K-means for each seed\n","for rseed in seeds:\n","    \n","    # initialize random centers\n","    centers = randomCenters(X, n_clusters, rseed)\n","    \n","    # output the K-means clustering\n","    labels, new_centers = kMeansClustering(X, n_clusters, centers)\n","\n","    # plot the final output of different seeds\n","    plotCenter(X, new_centers, labels, rseed, \"Cluster output with seed: \")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qknz1sGXHeO","colab_type":"text"},"source":["**Your Turn:** Execute the above code (for different random seeds value) and observe the results. Does the result looks same all the time? Can it successfully identify all the clusters?"]},{"cell_type":"markdown","metadata":{"id":"VLGeDl9VDQf5","colab_type":"text"},"source":["**Your Turn (Question 5):** Does the algorithm successfully identify the clusters each time?\n","\n","_Choose from: Yes, No_"]},{"cell_type":"markdown","metadata":{"id":"1RNCXf-lDY63","colab_type":"text"},"source":["**Your Turn (Question 6):** Explain why we got this type of output.\n","\n","_Replace with your answer_"]},{"cell_type":"markdown","metadata":{"id":"4dYnZoz6Tc0v","colab_type":"text"},"source":["**Your Turn:** In the above implementation, we stop when the `new_centers` of the clusters are the same as the previous step, `centers`; i.e. if the center of the cluster doesn't change from previous step, we stop and return those as the final cluster centroid. \n","\n","Let's assume, we could not identify the clusters successfully and got the output as the figure shown below. What if we **remove** that part of code (remove the `break`) and continue to run the code till `max_iter`?\n","\n","![alt text](https://www.comp.nus.edu.sg/~neamul/Images/week12.PNG)"]},{"cell_type":"markdown","metadata":{"id":"hoekT_IcDmrR","colab_type":"text"},"source":["**Your Turn (Question 7):** What result will we get, if we run the code for `max_iter` iterations?\n","\n","_Choose from: The result will be better than previous; The result will get worse than previous; The result may get better if we increase the value of `max_iter`; It will remain same as previous, no matter how many times we run_"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0T8yJ83MEukf"},"source":["## 3 Apply $k$-means on Moon dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QGy3BduIEukk"},"source":["We have seen two different version of $k$-means so far. Now we will apply our algorithm to a completely new dataset; the moon dataset. Let's generate the moon dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"erl39IxREukm","colab":{}},"source":["from sklearn.datasets import make_moons\n","# Generate moon dataset\n","Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n","# Plot the data points\n","plt.scatter(Xmoon[:, 0], Xmoon[:, 1], s=50);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MFmDzmZSEukv"},"source":["You can try both `sklearn` version of $k$-means or our implemented one and see the output."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ap5NysgEEukw","colab":{}},"source":["from sklearn.cluster import KMeans\n","## To run the sklearn version \n","\n","labels = KMeans(2, random_state=0).fit_predict(Xmoon)\n","plt.scatter(Xmoon[:, 0], Xmoon[:, 1], c=labels,s=50, cmap='viridis');\n","plt.suptitle(\"k-means on moon data : sklearn version\")\n","plt.show()\n","plt.close()\n","n_clusters = 2\n","\n","# Our implemented version\n","rand_seed = 11\n","centers = randomCenters(Xmoon, n_clusters, rand_seed)\n","labels, new_centers = kMeansClustering(Xmoon, 2, centers)\n","plotCenter(Xmoon, new_centers, labels, 11, \"k-means on moon data with seed: \")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F8kcTAsfFf-z","colab_type":"text"},"source":["**Your Turn (Question 8):** How can we improve the result for moon dataset?\n","\n","_Replace with your answer_"]},{"cell_type":"markdown","metadata":{"id":"7707XfmvlB2Q","colab_type":"text"},"source":["---\n","# Week 11: Post-tutorial Work\n","\n","Watch week $11$ post-videos and go through the exercise in the notebook.\n","\n","_Note:_ we re-use certain variables, (e.g., $X$) in the post tutorial notebook.  Please try to run the required portion of the notebook before running certain blocks."]},{"cell_type":"markdown","metadata":{"id":"8vCdoStJG1bv","colab_type":"text"},"source":["## 4 Programming : $k$-means++"]},{"cell_type":"markdown","metadata":{"id":"F8UFrmDq3QrV","colab_type":"text"},"source":["As you can see from the pre-tutorial notebook, $k$-means does not return the correct output each time. Now we will try to modify our center intialization using the following $k$-means++ algorithm.\n","\n","**$k$ _means++ algorithm:_**\n","\n","$\n","\\mu_1 \\leftarrow \\text{randomly chosen} \\\\\n","\\textbf{for } i=2 \\text{ to } k \\text{ do:} \\\\\n","\\hspace{6mm} \\textbf{for } j = 1 \\text{ to } m \\text{ do:} \\\\\n","\\hspace{12mm}d^{(j)} \\leftarrow \\text{ min}_{i'<i} ||x^{(j)} - \\mu_{i'}||\n","\\hspace{6mm} \\text{ ## compute distance}\\\\ \n","\\hspace{6mm} P: p(j) = \\frac{d^{(j)}}{\\sum_j d^{(j)}}, j \\in \\{ 1, \\cdots, m \\}\\hspace{12mm} \\text{ ## make a probability distribution } \\\\\n","\\hspace{6mm}c \\sim P, c\\in \\{1, \\dots, m\\} \\hspace{6mm}\\text{##Use $P$ to pick a point as the center}\\\\\n","\\hspace{6mm}\\mu_k \\leftarrow x^{(c)} \\\\\n","$\n","\n","Run $k$-means using $\\mu_{1...k}$ as the initial centers.\n"]},{"cell_type":"markdown","metadata":{"id":"L1f3PjQn_XFy","colab_type":"text"},"source":["### .a Choosing initial centers"]},{"cell_type":"markdown","metadata":{"id":"uX3YEVO3vfsu","colab_type":"text"},"source":["We will now implement this code for our initial centers. We have written some part of the code, but we left out the part that calculates the distance and the probabilities.\n","\n","Finish the code block below with the necessary code."]},{"cell_type":"markdown","metadata":{"id":"oCKoxpMNQFc8","colab_type":"text"},"source":["**Your Turn (Question 1):** Complete the code below to initialize cluster centers according to $k$-means++ algorithm."]},{"cell_type":"code","metadata":{"id":"Pa83I92FG853","colab_type":"code","colab":{}},"source":["from scipy import stats\n","\n","def initialCenters(X, n_clusters, rand_seed):\n","    \"\"\"\n","    Args:\n","      X (N x 2) : input data points\n","      n_clusterts (int) : number of clusters\n","      rand_seed (int) : seed for random number generation\n","    Returns:\n","      centers (array of float) : initial centers based on probability proportional to dist\n","    \"\"\"\n","    \n","    # Length of our dataset\n","    sz = len(X)\n","    \n","    # Fixing the random seed to have same output for everyone\n","    random.seed(rand_seed)\n","    \n","    # Generate first center randomly\n","    pos = random.sample(range(1, sz), 1)\n","    centers = X[pos]\n","    \n","    for i in range(n_clusters-1):\n","        dist = np.zeros(sz)\n","        # The following two lines are just for initialization. You can remove those if you don't need\n","        prob = np.zeros(sz)\n","        prob[0] = 1\n","        #######################################\n","        # Your Turn: write your code here\n","        #\n","        # For each point:\n","        #    calculate distance for current centers (initially 1)\n","        #    get the minimum dist and store them in dist\n","        #\n","        # 1. Calculate sum of all distances\n","        # 2. Calculate probability from dist and sum_of_all_dist\n","        # 3. Store the probability in `prob` as np array\n","        #\n","        #######################################\n","        \n","        # Generate random number from our custom prob distribution \n","        custom = stats.rv_discrete(name='custm', values=(np.arange(sz), prob))\n","          \n","        pos = custom.rvs(size=1)\n","        # Generate new center using the generated random number and append it with previous centers\n","        centers = np.append(centers, [X[pos[0]]], axis = 0)\n","    \n","    return centers"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0v2Gd995lRJ","colab_type":"text"},"source":["### .b Setup\n","Let's generate our data points using `blobs`."]},{"cell_type":"code","metadata":{"id":"_1a7m8It-d9a","colab_type":"code","colab":{}},"source":["# import necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","\n","import seaborn as sns; sns.set()\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W1agkt7IFxTp","colab_type":"text"},"source":["### .c Run the $k$-means"]},{"cell_type":"markdown","metadata":{"id":"8xN9dWAm8Eg8","colab_type":"text"},"source":["Now we will try our $k$-means using this center initialization. Let's see if we can get better result or not.\n","\n","Make sure you run the **`kMeansClustering()`** function, that you implemented in **Section $2c$**. We are using the same implementation with modified center intialization."]},{"cell_type":"code","metadata":{"id":"9_xhxIQzjklI","colab_type":"code","colab":{}},"source":["# Assign number of clusters\n","n_clusters = 4\n","\n","\n","# Seed for random number generation for first center; you can change it to different value to see if it works or not\n","rand_seed = 19\n","\n","# Substitute in new center initialization\n","centers = initialCenters(X, n_clusters, rand_seed)\n","# print(centers)\n","# For reference, this was the old code block from the beginning that called the original version.\n","# centers = randomCenters(X, n_clusters, 11)\n","\n","# Using the same k means with new centers\n","labels, new_centers = kMeansClustering(X, n_clusters, centers)\n","\n","# Plot the final output\n","plotCenter(X, new_centers, labels, rand_seed, \"K-means++ with seed: \")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4MyJR71x3Db","colab_type":"text"},"source":["The output looks great. You can try it with any random seed, the result should be better than the basic $k$-means algorithm."]},{"cell_type":"markdown","metadata":{"id":"mTBQRbAolj9B","colab_type":"text"},"source":["## 5 Problems with $k$-Means Algorithm"]},{"cell_type":"markdown","metadata":{"id":"IFg9-RWm-l0J","colab_type":"text"},"source":["In this part we will look at a few **problems** of $k$-means and will try to improve the result using the **Gaussian Mixture Model**. In the pre-tutorial exercise, we have seen $k$-means can cluster the points pretty easily (within a few iterations). \n","\n"]},{"cell_type":"markdown","metadata":{"id":"i-rcKYAqTn_g","colab_type":"text"},"source":["### .a Run $k$-Means"]},{"cell_type":"markdown","metadata":{"id":"nsg3uD-5HzYt","colab_type":"text"},"source":["We will use the same dataset (from Section $4b$) to run the `sklearn` version of $k$-means. We have already did that previously, let's do it once more and discuss some problems!"]},{"cell_type":"code","metadata":{"id":"g_dgTNCnHJ02","colab_type":"code","colab":{}},"source":["# import sklearn library for kmeans\n","from sklearn.cluster import KMeans\n","# create an instance of kmeans with 4 clusters\n","kmeans = KMeans(4, random_state=0)\n","# gather cluster output for our data points\n","labels = kmeans.fit(X).predict(X)\n","# plot the cluster output\n","plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SU9xP1NzIDje","colab_type":"text"},"source":["### .b Problem with cluster assignment\n","Intuitively, we can see from the plot, the assignment of some points we are confident about the assignment of clusters of some points more than others. For example, we are more confident of assigning points that lie close to the center of the clusters correctly than the points lying close to other cluster boundary. From the $k$-means algorithm, we have seen that the algorithm doesn't have any measure of uncertainty or probability of cluster assignments. \n","\n","One easy way of thinking of $k$-means algorithm is that it places a circle (or hypersphere, in higher dimensions) at the center of the each cluster. The radius of those circles is defined by the most distant point in the cluster. This radius is a **hard** cut-off for a cluster; _i.e._, any point that lies inside the radius is included in the cluster; otherwise, it is not. \n","\n","We can visualize the cluster boundaries using circles. There is a slight overlap between the two clusters in the middle, which we can't see from the above figure. We will try to visualize it using the circles."]},{"cell_type":"code","metadata":{"id":"4KlOOr3y4mHg","colab_type":"code","colab":{}},"source":["from scipy.spatial.distance import cdist\n","\n","def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n","    \"\"\"\n","      Args:\n","        kmeans : the instance of our kmeans classifier\n","        X (m x 2) : input data points\n","        n_clusters (int) : number of clusters\n","        rseed (int) : seed for random number\n","        ax : axis of the data points for plotting\n","    \"\"\"\n","    # Get the cluster assignment\n","    labels = kmeans.fit_predict(X)\n","\n","    # plot the input data\n","    ax = ax or plt.gca()\n","    ax.axis('equal')\n","    #ax.set_xlim(-5,5)\n","    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n","\n","    # plot the representation of the KMeans model\n","    centers = kmeans.cluster_centers_\n","    radii = [cdist(X[labels == i], [center]).max() for i, center in enumerate(centers)]\n","    # Different hexcodes for different clusters\n","    hexCodes = ['#CCEEBB', '#CCAACC', '#CCFFDD', '#AABBCC']\n","    for c, r, clr in zip(centers, radii, hexCodes):\n","        ax.add_patch(plt.Circle(c, r, fc=clr, lw=3, alpha=0.5, zorder=1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijItJEGc4svX","colab_type":"code","colab":{}},"source":["# Create an instance of k means for 4 clusters\n","kmeans = KMeans(n_clusters=4, random_state=0)\n","# Plot the cluster with boundary\n","plot_kmeans(kmeans, X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6aAjnPCNyiF","colab_type":"text"},"source":["Note: The points may seem a bit closer than the original data points. It is because of the scaling of our figure, we now have equal sized axes now to visualize the circle better. Previously, the range for both axes were different. The data points are actually same, only the range of the figure changed."]},{"cell_type":"markdown","metadata":{"id":"lpwldRyKJjlW","colab_type":"text"},"source":["### .c $k$-means on elliptical clusters\n","From the plot above, it is visible that there is a slight overlap between the two clusters in the middle. Now, think of a point which is equidistant from two or more clusters. As we implemented our $k$-means algorithm, we first calculate distance from a cluster and then we check if it is less than previous minimum. If the distance is strictly less than other centers we assign the point to that cluster. That means that if we have a point equidistant from two clusters, it will always be assigned to the first cluster. \n","\n","Is it necessary correct way to assign a point to a cluster when there is a point that is equidistant from both the clusters? Intuitively, we would think that it is equally likely for the point to belong to both clusters. The $k$-means algorithm doesn't consider fully capture the dilemma in this scenario. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"mTjdz3CPV2Vk","colab_type":"text"},"source":["\n","Another observation from the above plot, $k$-means always think the clusters are **circular**, it doesn't take into consideration that the cluster can be of other shapes such as *ellipse* or *oblong*.\n","\n","We can use the same data and transform it to be stretched out. Then we apply $k$-means and visualize the clusters. From the plot, you can understand the point above better."]},{"cell_type":"code","metadata":{"id":"7MoH-ZE15Li4","colab_type":"code","colab":{}},"source":["# Transform the original data points to more stretched points\n","rng = np.random.RandomState(13)\n","X_stretched = np.dot(X, rng.randn(2, 2))\n","\n","# Get clustering for the transformed data points\n","kmeans = KMeans(n_clusters=4, random_state=0)\n","\n","# Plot the cluster output\n","plot_kmeans(kmeans, X_stretched)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3b_tgG0AKZcO","colab_type":"text"},"source":["In the figure above, there is a huge overlap between the clusters, which is not good. When the clusters doesn't form a circular shape, $k$-means doesn't seem to perform poorly. So we need a clustering method that is more general, not limited to only circles. Instead of assigning to the closest cluster, we may consider the distances of each point to all cluster centers. In the next section, we will try to use this ideas to improve our cluster assignments."]},{"cell_type":"markdown","metadata":{"id":"vc6x-IWgW6nk","colab_type":"text"},"source":["**Your Turn (Question 2):** Which of the following are disadvantages of $k$-means?\n","\n","_Choose from: Easy to implement; Lack of flexibility in cluster shape; With huge number of variable, computing $k$ means costs more; Doesn't have probabilistic assignment_"]},{"cell_type":"markdown","metadata":{"id":"PRVOA6bo6dmP","colab_type":"text"},"source":["## 6 Gaussian Mixture Models"]},{"cell_type":"markdown","metadata":{"id":"7aFdb3cqNO_X","colab_type":"text"},"source":["In this section, we will learn *Gaussian Mixture Model* (GMM), which attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bj75ZMNj7s8x","colab_type":"text"},"source":["### .a GMM for clustering\n","\n","Let's first use the `GMM` package from `sklearn` for finding clusters (in an identical manner as using $k$-means)."]},{"cell_type":"code","metadata":{"id":"Umyod9kU6hTf","colab_type":"code","colab":{}},"source":["# import libraries for GMM\n","from sklearn import mixture\n","\n","# Create an instance of GMM with 4 components\n","gmm = mixture.GaussianMixture(n_components=4).fit(X)\n","\n","# Get cluster assignment for input data\n","labels = gmm.predict(X)\n","\n","# Plot the cluster output\n","plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SiRxGMHYzKi","colab_type":"text"},"source":["### .b Probability based cluster assignment"]},{"cell_type":"markdown","metadata":{"id":"NZvxhvsrsfjY","colab_type":"text"},"source":["\n","The output looks similar to what we had previously, right? But it's actually not. Though `GMM` finds cluster like $k$-means, but `GMM` contains a probabilistic model under the hood. `GMM` uses probabilistic cluster assignment. For each point, it has probability of assigning it to different clusters.\n","\n","We can look at some of these probabilities for a few points using `sklearn`."]},{"cell_type":"code","metadata":{"id":"eLUw77uR7sQA","colab_type":"code","colab":{}},"source":["# Get prediction probability of input data points\n","probs = gmm.predict_proba(X)\n","# Print probability of first 10 points\n","print(probs[:10].round(3))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2MOsIJcx5H6","colab_type":"text"},"source":["From the above output, we can see the `probability matrix` of first $10$ points. For each point, it gives a row vector that specifies the probability of assigning the point to each of the 4 different clusters.\n","\n","So till now, we have just talked about how `GMM` assigns a point to a cluster. But from the output we cannot distinguish. Let's know some more details about a *Gaussian Mixture Model*."]},{"cell_type":"markdown","metadata":{"id":"5DdebolMYeT-","colab_type":"text"},"source":["**Your Turn (Question 3):** What happens when a sample have $0.5$ probability for two clusters?\n","\n","_Choose from: It will be considered as outlier, We will ignore this point in our clustering, It will be assigned to both cluster, We will assign it to any one of these two, We will assign it to any one random cluster among all_"]},{"cell_type":"markdown","metadata":{"id":"XEjiUiVrYwka","colab_type":"text"},"source":["### .c Gaussian Mixture Model : Expectation-Maximization"]},{"cell_type":"markdown","metadata":{"id":"ND3e3cFLtdVf","colab_type":"text"},"source":["\n","\n","A Gaussian mixture model is very similar to $k$-means, in the sense that it uses an expectation-maximization approach which follows the following steps:\n","\n","1. Choose starting guesses for the location and shape\n","2. Repeat until convergence:\n","\n","    a) *E-step*: for each point, find weights encoding the probability of membership in each cluster;\n","    \n","    b) *M-step*: for each cluster, update its location, normalization and shape, based on all data points in the cluster, making use of the weights.\n","    \n","The output of this algorithm is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model. Just like in $k$-means, this algorithm may also sometime miss the global optimum, depending on the initialization of the centers.\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"CcwRbU3HyiDA","colab_type":"text"},"source":["### .d Visualize GMM Cluster boundaries\n","The function below will help us to visualize the locations and shapes of the `GMM` clusters by drawing ellipses based on the `GMM` output."]},{"cell_type":"code","metadata":{"id":"CgPcGrmB7yfe","colab_type":"code","colab":{}},"source":["from matplotlib.patches import Ellipse\n","\n","def draw_ellipse(position, covariance, ax=None, **kwargs):\n","    \"\"\"Draw an ellipse with a given position and covariance\n","      Args:\n","        position : given position of the cluster\n","        covariance: covariance matrix of GMM\n","        ax : axis of the data points for plotting\n","        kwargs : weights of the cluster points\n","        \n","    \"\"\"\n","    ax = ax or plt.gca()\n","    \n","    # Convert covariance to principal axes\n","    if covariance.shape == (2, 2):\n","        U, s, Vt = np.linalg.svd(covariance)\n","        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n","        width, height = 2 * np.sqrt(s)\n","    else:\n","        angle = 0\n","        width, height = 2 * np.sqrt(covariance)\n","    \n","    # Draw the ellipse\n","    for nsig in range(1, 4):\n","        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))\n","        \n","def plot_gmm(gmm, X, label=True, ax=None):\n","    \"\"\"\n","      Visualize the output of a fitted GMM\n","      Args:\n","        gmm : instance of GMM classifier\n","        X (N x 2): input data points\n","    \"\"\"\n","    ax = ax or plt.gca()\n","    labels = gmm.fit(X).predict(X)\n","    if label:\n","        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n","    else:\n","        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n","    ax.axis('equal')\n","    \n","    w_factor = 0.2 / gmm.weights_.max()\n","    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n","        draw_ellipse(pos, covar, alpha=w * w_factor)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6lxqWt6TFCGO","colab_type":"text"},"source":["Let's plot the `GMM` cluster output using the above helper functions."]},{"cell_type":"code","metadata":{"id":"RSKCQJGk70qO","colab_type":"code","colab":{}},"source":["# Creating an instance of GMM with 4 components\n","gmm = mixture.GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n","# Plot output of GMM\n","plot_gmm(gmm, X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3tCBFnY8zIgc","colab_type":"text"},"source":["### .e GMM on Elliptical Clusters\n","We have seen that $k$-means cannot fit the same points, when we transform them and make them stretched. For stretched points, the clusters overlap with each other a lot. Now we will try to fit those same (stretched) points using `GMM`. Let's see if `GMM` can fit these points better than $k$-means or not."]},{"cell_type":"code","metadata":{"id":"2tcQEVWd8JRH","colab_type":"code","colab":{}},"source":["# Creating GMM instance with covariance type full; you can also use spherical or diag\n","gmm = mixture.GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n","# Plot the output of GMM\n","plot_gmm(gmm, X_stretched)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1QQOH2x0Gh-","colab_type":"text"},"source":["Great! The output looks way better than $k$-means. Because `GMM` doesn't try to fit the data in circles, as we can see the output it fits the data using **ellipse**. That is also why `GMM` performs better than $k$-means."]},{"cell_type":"markdown","metadata":{"id":"J9YaDGfOahpe","colab_type":"text"},"source":["Now, we have learned two different ways of cluster assignment."]},{"cell_type":"markdown","metadata":{"id":"V99WzYj1aqYd","colab_type":"text"},"source":["**Your Turn (Question 4):** What is the main difference between *hard clustering* and *soft clustering*?\n","\n","_Replace with your answer_"]},{"cell_type":"markdown","metadata":{"id":"rKZcbCTea0D8","colab_type":"text"},"source":["**Your Turn (Question 5):** Which one *(Hard clustering and soft clustering)* is better and why? \n","\n","_Replace with your answer_"]},{"cell_type":"markdown","metadata":{"id":"4GdrCiLt11oe","colab_type":"text"},"source":["## 7 Application of GMM"]},{"cell_type":"markdown","metadata":{"id":"8pZseP8S18V-","colab_type":"text"},"source":["In the previous section, we have used `GMM` for clustering, because it is a clustering algorithm. But `GMM` is also used for *density estimation*; i.e. estimating the underlying distribution of the sample points, which we can use to generate new points.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OPf-MhMO-jVo","colab_type":"text"},"source":["### .a Setup\n","We will see this application of `GMM` with the two moons dataset. Let's generate the dataset first."]},{"cell_type":"code","metadata":{"id":"3j5tlxFG8pNn","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_moons\n","# Generate moon data points\n","Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n","# Plot the data points\n","plt.scatter(Xmoon[:, 0], Xmoon[:, 1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVBf7rVQ2gss","colab_type":"text"},"source":["### .b Learning the density function\n","If we apply `GMM` with only **two components**, the output looks poor (In the previous example, we have used 4 components, because we had four clusters, whereas we now have only two)."]},{"cell_type":"code","metadata":{"id":"Z9EVX4UQ8rjn","colab_type":"code","colab":{}},"source":["# Create instance of GMM with 2 components; same as before\n","gmm2 = mixture.GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n","\n","# Plot output of GMM\n","plot_gmm(gmm2, Xmoon)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VerNqNjm218S","colab_type":"text"},"source":["So the output doesn't seem to fit the data so well. Its because we have used only two components to estimate the density of the data. Now we will try to increase the number of components and see if our model can fit the underlying distribution or not. We will be using $16$ components for the moon dataset."]},{"cell_type":"code","metadata":{"id":"TCrVm89S8yHW","colab_type":"code","colab":{}},"source":["# Create instance of GMM with 16 components\n","gmm16 = mixture.GaussianMixture(n_components= 16, covariance_type='full', random_state=0)\n","# Plot the output of GMM\n","plot_gmm(gmm16, Xmoon, label=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_l6fIVMC3td4","colab_type":"text"},"source":["From, the above output, it seems like our model has learned most of the data points. But from this figure we cannot judge how well it can learn the density function. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"xY11PiJR-vID","colab_type":"text"},"source":["### .c Generating new points\n","We will generate 1000 points using the learned `GMM` model, and compare the generated points with the original dataset."]},{"cell_type":"code","metadata":{"id":"hR-lY-bN9ZEf","colab_type":"code","colab":{}},"source":["# Generate new data points from the learned GMM\n","Xnew=gmm16.sample(n_samples=1000)\n","# Plot generated data points\n","plt.scatter(Xnew[0][:, 0], Xnew[0][:, 1]);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BdvdTGQB4JM_","colab_type":"text"},"source":["It looks quite good. Our model seems to have learned the distribution of the original dataset. This is a very useful application of `GMM`.  We can use this model to generate data points of an unknown distribution, which may help us in our learning algorithms."]},{"cell_type":"markdown","metadata":{"id":"aZGpFGdwaAsT","colab_type":"text"},"source":["**Your Turn (Question 6):** In which of the following situation will generating more data points will be helpful?\n","\n","_Choose from: Detecting expression from comments on social media, Detecting useful keyword from news blogs, Cancer detection where we have lots of negative data points only a few positive points_"]},{"cell_type":"markdown","metadata":{"id":"TsEf2xy6Itb4","colab_type":"text"},"source":["**N.B.** The post exercise is adapted from Python Data Science NoteBook on [Gaussian Mixture Model](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"-kGaO2wXmyVP","colab_type":"text"},"source":["---\n","# Credits\n","Authored by Mohammad Neamul Kabir and  [Min-Yen Kan](http://www.comp.nus.edu.sg/~kanmy) (2019), affiliated with [WING](http://wing.comp.nus.edu.sg), [NUS School of Computing](http://www.comp.nus.edu.sg) and [ALSET](http://www.nus.edu.sg/alset).\n","Licensed as: [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/ ) (CC BY 4.0).\n","Please retain and add to this credits cell if using this material as a whole or in part.\n","Other Credits (inclusive of photos): "]}]}