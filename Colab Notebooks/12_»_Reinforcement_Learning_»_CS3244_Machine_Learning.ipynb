{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "12 » Reinforcement Learning » CS3244 Machine Learning",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOG-Q8v7vGGh",
        "colab_type": "text"
      },
      "source": [
        "Available at http://www.comp.nus.edu.sg/~cs3244/1910/12.colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeKMuBl4uxXT",
        "colab_type": "text"
      },
      "source": [
        "![Machine Learning](https://www.comp.nus.edu.sg/~cs3244/1910/img/banner-1910.png)\n",
        "---\n",
        "See **Credits** below for acknowledgements and rights.  For NUS class credit, you'll need to do the corresponding _Assessment_ in [CS3244 in Coursemology](http://coursemology.org/courses/1677) by the respective deadline (as in Coursemology). \n",
        "\n",
        "**You must acknowledge that your submitted Assessment is your independent work, see questions in the Assessment at the end.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ_n0YfHvMaO",
        "colab_type": "text"
      },
      "source": [
        "**Learning Outcomes for this Week**\n",
        "- Have a basic understanding of Reinforcement Learning and the key terms used.\n",
        "- Be introduced to Gym, a Reinforcement Learning library.\n",
        "- Implement a policy to solve  _MountainCar_, a classic Reinforcement Learning problem.\n",
        "- Learn about Markov Decision Processes (MDP).\n",
        "- Find the optimal policy for an MDP by implementing the Value Iteration algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOG6Uko1vNYL",
        "colab_type": "text"
      },
      "source": [
        "_Welcome to the Week 12 Python notebook._ This week we will learn about the domain of Reinforcement Learning.  We introduce the basic **concepts of Reinforcement Learning** and **MDP** in the lecture videos, and will be reviewing this material in tutorial $10$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NrXkAoYjeLaB"
      },
      "source": [
        "---\n",
        "# Week 12: Pre-Tutorial Work\n",
        "\n",
        "* Watch the CS 3244 video playlist for Week 12 Pre, which will introduce the main concepts of Reinforcement Learning.\n",
        "* After watching the videos, complete the pre-tutorial exercises and questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlSUdxL4XZDU",
        "colab_type": "text"
      },
      "source": [
        "## 1 Introduction to Reinforcement Learning\n",
        "\n",
        "Reinforcement learning is a ML paradigm that focused on finding the most suitable action in order to maximize 'reward' in a particular situation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laK71DLLZBab",
        "colab_type": "text"
      },
      "source": [
        "### .a General Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rFsQwZe6OQkZ"
      },
      "source": [
        "\n",
        "The general framework can be described as follows (also refer to the illustration below):\n",
        "  \n",
        "> 1. At time $t$, the **agent** makes the decision on which **action** $A_t$ to take, based on the **reward** $R_t$ and the **observed enviroment** $O_t$.\n",
        ">\n",
        "> 2. Upon recieving the action $A_t$, the **enviroment** gives the next observation $O_{t+1}$ and reward $R_{t+1}$ to the agent.\n",
        ">\n",
        "> 3. As objective of the agent is to maximize total future reward, the agent may have to sacrifice immediate reward to gain a larger long-term reward. \n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://www.comp.nus.edu.sg/~neamul/Images/CS3244_1910/RL-in-a-nutshell.png\"  width=\"600\">\n",
        "</div>\n",
        "\n",
        "_(Diagram credit:Sutton, R. S. and Barto, A. G. Introduction to Reinforcement Learning)_\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnGKNMjXmoWQ",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 1)**: The observed enviroment $O_t$ may differ from the actual enviroment. In which of the following reinforcement learning scenarios this **does NOT** happen? \n",
        "\n",
        "_Choose from: An agent that plays [connect four](https://en.wikipedia.org/wiki/Connect_Four); Driving NUSmart Shuttle; Making a robot to [perform gymnastics](https://www.youtube.com/watch?v=YdnJI9T-yXI)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5ilW5oNZFfd",
        "colab_type": "text"
      },
      "source": [
        "### .b Components of RL Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx3AXZPmraA3",
        "colab_type": "text"
      },
      "source": [
        "Most RL agents also have the following  components: \n",
        "*   **Policy** - It is a map $\\pi$ from state to action. This can be thought of the agent's behaviour and could either be deterministic or stochastic.\n",
        "*   **Value Function** - It gives a prediction of the future reward. This can be used to evaluate how good each state/action is. \n",
        "*   **Model** - The agent's representation of the enviroment. It is a prediction of the next state and next immediate reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ8NGYa4moih",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 2)**: Let $\\pi$ be a deterministic policy and suppose at time $t$, the agent has taken action $A_t=a$. Is it true that $R_{t+1}$ is [deterministic](https://en.wikipedia.org/wiki/Deterministic_algorithm)?\n",
        "\n",
        "_Choose from: No, as the state space might be non-deterministic; No, as policy changes are quite frequent; Yes, as agent's action at time $t$ is hard-coded_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zehAFycwQFS",
        "colab_type": "text"
      },
      "source": [
        "Another important concept in reinforcement learning is the tradeoff between exploration and exploitation. In other words, the agent should discover a good policy from its experiences of the enviroment without losing too much reward along the way!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3cA3-3g5_ZRI"
      },
      "source": [
        "**Your Turn (Question 3)**: Suppose a reinforcement learning agent $N$ has adopted policy $\\mathscr{I}$ for quite some time (roughly $10$ years) with high reward. Now, agent $N$ suddenly chooses to adopt policy $\\mathscr L$. What is one possible reason for adopting policy $\\mathscr L$?\n",
        "\n",
        "_Choose from: Agent is attempting to explore the state space; There is no reason to change the policy as the agent is recieving a high reward; The agent is tired of receiving high reward_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W3UpLR1OeLaa"
      },
      "source": [
        "## 2 OpenAI Gym\n",
        "\n",
        "Let us introduce [OpenAI Gym](https://gym.openai.com). Gym is a python library that wraps many classical decision problems including robot control, video-games and board games. We will use this to illustrate a simple example of how *state-action-agent-observation-reward* can be implemented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6AhzzdqZWmw",
        "colab_type": "text"
      },
      "source": [
        "### .a Get Necessary Libraries\n",
        "\n",
        "First we install some necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "coh3Y88leLab",
        "colab": {}
      },
      "source": [
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install pyglet==v1.3.2\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbkdbDgAh9E8",
        "colab_type": "text"
      },
      "source": [
        "### .b MountainCar Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQTfowmPUSQE",
        "colab_type": "text"
      },
      "source": [
        "[MountainCar](https://gym.openai.com/envs/MountainCar-v0/) is a classic RL problem. The scenario is as follows:\n",
        "\n",
        "A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right to reach the flag. However, the car's engine is not strong enough to scale the mountain in a single pass. \n",
        "\n",
        "Play the video below to see the scenario in action!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZjP6mRxa3US",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HTML(\"\"\"\n",
        "<video width=\"400\" height=\"400\" controls>\n",
        "  <source src=\"https://www.comp.nus.edu.sg/~neamul/Images/CS3244_1910/mountain_car_video.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcYbAeaZiGXX",
        "colab_type": "text"
      },
      "source": [
        "### .c Gym interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLshLDS3bvzB",
        "colab_type": "text"
      },
      "source": [
        "In Gym, the <code>Env</code> class \"models\" the enviroment. There are three main methods in <code>Env</code>:\n",
        "* __reset()__ - resets environment to initial state; _returns first observation_\n",
        "* __render()__ - shows current environment state (a more colorful version ^\\_^ )\n",
        "* __step(a)__ - commits action __a__; returns a tuple (new observation, reward, is done, info)\n",
        " * _new observation_ - an observation right after commiting the action __a__\n",
        " * _reward_ - a number representing your reward for commiting action __a__\n",
        " * _is done_ - True if the goal is attained, False if still in progress\n",
        " * _info_ - some auxilary stuff about what just happened. For our purposes we can ignore it.\n",
        "\n",
        "\n",
        "You can check out the documentation [here](https://github.com/openai/gym/blob/master/gym/core.py) for more details.\n",
        "\n",
        "Our first step is to create the enviroment. Run the code block below to do just that. \n",
        "\n",
        "Optional: You can try (later) replacing <code>\"MountainCar-v0\"\n",
        "</code> with <code>\"CartPole-v0\"</code> or <code> \"MsPacman-v0\"</code>\n",
        "</code> in the following code block to see other enviroments supported by Gym!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bDT2SkAjeLap",
        "colab": {}
      },
      "source": [
        "# Create environment for MountainCar\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Set a fixed seed before each call to reset\n",
        "SEED_NUM = 2019 \n",
        "env.seed(SEED_NUM) \n",
        "obs0 = env.reset()\n",
        "\n",
        "# Plot the initial environment\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "print(\"Initial observation: [ %f , %f ]\" % (obs0[0], obs0[1]) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws_rDp_snb-w",
        "colab_type": "text"
      },
      "source": [
        "Notice that we have also printed the observation space and action space of our enviroment. They are `Box(2,)` and `Discrete(3)` respectively. \n",
        "\n",
        "* The `Box(n,)` space represents an `n`-dimensional box. For us, this means that valid observations will be an array of $2$ numbers.\n",
        "\n",
        "* The `Discrete(n)` space allows a fixed range (e.g. `0,1, . . ., n-1`) of non-negative numbers. In this case, valid actions are either $0$, $1$ or $2$.\n",
        "  \n",
        "The above code also prints an example of an observation- the initial observation. Try changing the value of `SEED_NUM` and observe the changes in the printed observation and the illustration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj3uIvrGb75q",
        "colab_type": "text"
      },
      "source": [
        "### .d Performing an Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX60WSsrraww",
        "colab_type": "text"
      },
      "source": [
        "For the `MountainCar-v0` problem, an observation is the tuple (`position of the car`, `velocity of the car`). Let us commit action `0` so we can try to guess what the action is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pM7ANVjrkiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED_NUM = 2019\n",
        "env.seed(SEED_NUM)\n",
        "\n",
        "obs0 = env.reset()\n",
        "print(\"Initial observation: [ %f , %f ]\" % (obs0[0], obs0[1]) )\n",
        "\n",
        "# Try an action\n",
        "act = 0\n",
        "print(\"Performing action %d\" % act)\n",
        "new_obs, reward, is_done, _ = env.step(action=act)\n",
        "print(\"New observation: [ %f , %f ]\" % (new_obs[0], new_obs[1]) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raw3oqGgqzRY",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 4)**: What does action `2` represent?\n",
        "\n",
        "_Choose from: Drive right; Drive left; Do nothing_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui2pw6N2dOA1",
        "colab_type": "text"
      },
      "source": [
        "### .e A Simple Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Q-uAuXfhAw",
        "colab_type": "text"
      },
      "source": [
        "Recall that a policy is a map from state to action. We can write a simple (deterministic) policy in Gym as follows. \n",
        "\n",
        "**N.B:** Note that \"time\" (`t`) is not stored in state!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgHpNl2ehxkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_policy(t, s): \n",
        "    \"\"\"\n",
        "    Args:\n",
        "      t (int) : time step\n",
        "      s (list of floats) : the state (or observation)- consists of the position and velocity\n",
        "\n",
        "    Returns:\n",
        "      action (int) : Returns an action- 0, 1 or 2 \n",
        "    \"\"\"\n",
        "    initial_position = -0.534186 # Initial position when SEED_NUM = 2019\n",
        "\n",
        "    if s[0] < initial_position: \n",
        "        return 0\n",
        "    else:\n",
        "        return 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXiPTiIEjd3L",
        "colab_type": "text"
      },
      "source": [
        "We can visualize what `sample_policy` does by using the code block below. Run all three code blocks below to generate the animation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFyrMvXaj_Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TIME_LIMIT = 250 # Maximum number of steps (time units) allowed\n",
        "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(), max_episode_steps=TIME_LIMIT + 1)\n",
        "\n",
        "SEED_NUM = 2019\n",
        "env.seed(SEED_NUM)\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "s = env.reset()\n",
        "frames = []\n",
        "frames.append(env.render(mode = 'rgb_array'))\n",
        "\n",
        "for t in range(TIME_LIMIT):\n",
        "    # Perform the action given by the policy, and get the updated enviroment  \n",
        "    action = sample_policy(t, s)\n",
        "    s, r, done, info = env.step(action)\n",
        "    \n",
        "    frames.append(env.render(mode = 'rgb_array'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC4DU7vqeBIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# The above comment discards the output of this code block (but can be used to save it to a variable too)\n",
        "\n",
        "# Generate the animation\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYMX87iWRyBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UWgK7eBJeLa9"
      },
      "source": [
        "### .f Reaching the flag\n",
        "\n",
        "As demostrated by `sample_policy` above, the car's engine is not strong enough to overcome gravity and scale the mountain in one pass. \n",
        "\n",
        "__Your task__ is to find a strategy that reaches the flag. \n",
        "\n",
        "You're not required to build a sophisticated algorithm. So, feel free to hard-code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0pp98QOc9Lp",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 5):** Complete the code below for `my_policy` function, to allow the car to reach the flag.\n",
        "\n",
        "_Copy the code you added or modified in the assessment_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk-lJ27tX7p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ok, we'll give you the meaning of the three valid actions that are applicable to this problem.\n",
        "# For those learners that try the other Gym environments, note that these scalar values\n",
        "# would denote different actions that those listed here for this MountainCar-v0 problem. \n",
        "actions = {'left': 0, 'stop': 1, 'right': 2}\n",
        "\n",
        "def my_policy(t, s):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      t (int) : time step\n",
        "      s (list of floats) : the state (or observation)- consists of the position and velocity\n",
        "\n",
        "    Returns:\n",
        "      action (int) : Returns an action- 0, 1 or 2 \n",
        "    \"\"\"\n",
        "\n",
        "    ######################################################\n",
        "    # Your Turn (Q5): write your own code here\n",
        "    #\n",
        "    # Fill in the return values for this policy to make the car reach the flag.\n",
        "    # It should return an action from the `actions` dict.\n",
        "    #\n",
        "    # You do not need to add a new condition, just fill in the return statements.\n",
        "    #\n",
        "    position = s[0]\n",
        "    velocity = s[1]\n",
        "\n",
        "    if t < 30:\n",
        "        return # Your Turn: return an action\n",
        "    \n",
        "    if velocity < 0 and position > 0:\n",
        "        return # Your Turn: return an action\n",
        "    \n",
        "    return # Your Turn: return an action\n",
        "    ######################################################    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7McSyg9rYQwP",
        "colab_type": "text"
      },
      "source": [
        "Run the code blocks below to see if the car successfully reaches the flag using `my_policy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6qqKF6treLbA",
        "colab": {}
      },
      "source": [
        "# Don't change these values\n",
        "TIME_LIMIT = 250 # Maximum number of steps (time units) allowed\n",
        "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(), max_episode_steps=TIME_LIMIT + 1)\n",
        "\n",
        "SEED_NUM = 2019\n",
        "env.seed(SEED_NUM)\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "s = env.reset()\n",
        "frames = []\n",
        "frames.append(env.render(mode = 'rgb_array'))\n",
        "\n",
        "for t in range(TIME_LIMIT):  \n",
        "    # Perform the action given by the policy, and get the updated enviroment  \n",
        "    action = my_policy(t,s)\n",
        "    s, r, done, info = env.step(action) \n",
        "\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # print(\"time\", t, \"action\", action, \"postion\", s[0], \"velocity\", s[1])\n",
        "\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzlqMmPqtw4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Generate the animation\n",
        "env.render()\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq-RY7t7R3wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the output\n",
        "if done:\n",
        "    print(\"You solved it!\")\n",
        "else:\n",
        "    print(\"Time limit exceeded. Try again.\")\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDhVqUeOtF50",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Week 12: Post-Tutorial Work\n",
        "Watch the Week 12 post-videos on the lecture topics introduced this week, and then attempt the following exercises.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPnBwLpfI8d",
        "colab_type": "text"
      },
      "source": [
        "## 3 Markov decision process\n",
        "A Markov decision process (MDP) formally describes an enviroment for reinforcement learning. This section of the notebook deals with implementing a MDP and finding an optimal policy via an iterative algorithm called **Value Iteration.**\n",
        "\n",
        "First, let us recap some key terms. A MDP comprises of a\n",
        "\n",
        "*   finite collection of **states**\n",
        "*   finite collection of **actions**\n",
        "*   **transition probabilities** between any two states\n",
        "*   a **reward** associated to each action\n",
        "*   **discount factor** $\\gamma$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GhHmBrG6LpN",
        "colab_type": "text"
      },
      "source": [
        "### .a Setup\n",
        "\n",
        "The hidden code cell below defines a custom MDP class (which we can use like an OpenAI Gym environment!) and some functions for visualisation. Let's run the code block below. _You don't need to understand the code. If you want to look at it, double click on the code block._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3PqGiTLfX4y",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# Code from Practical Reinforcement Learning on Coursera https://www.coursera.org/learn/practical-rl/\n",
        "\n",
        "\"\"\"\n",
        "Implements the following:\n",
        "- MDP class\n",
        "- plot_graph \n",
        "- plot_graph_with_state_values\n",
        "- plot_graph_optimal_strategy_and_state_values\n",
        "\"\"\"\n",
        "\n",
        "# Most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from gym.utils import seeding\n",
        "\n",
        "try:\n",
        "    from graphviz import Digraph\n",
        "    import graphviz\n",
        "    has_graphviz = True\n",
        "except ImportError:\n",
        "    has_graphviz = False\n",
        "\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
        "        \"\"\"\n",
        "        Defines an MDP. Compatible with gym Env.\n",
        "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
        "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
        "            For each state and action, probabilities of next states should sum to 1\n",
        "            If a state has no actions available, it is considered terminal\n",
        "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
        "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
        "            The reward for anything not mentioned here is zero.\n",
        "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
        "            By default, picks initial state at random.\n",
        "\n",
        "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
        "\n",
        "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
        "        transition_probs = {\n",
        "              's0':{\n",
        "                'a0': {'s0': 0.5, 's2': 0.5},\n",
        "                'a1': {'s2': 1}\n",
        "              },\n",
        "              's1':{\n",
        "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "                'a1': {'s1': 0.95, 's2': 0.05}\n",
        "              },\n",
        "              's2':{\n",
        "                'a0': {'s0': 0.4, 's1': 0.6},\n",
        "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
        "              }\n",
        "            }\n",
        "        rewards = {\n",
        "            's1': {'a0': {'s0': +5}},\n",
        "            's2': {'a1': {'s0': -1}}\n",
        "        }\n",
        "        \"\"\"\n",
        "        self._check_param_consistency(transition_probs, rewards)\n",
        "        self._transition_probs = transition_probs\n",
        "        self._rewards = rewards\n",
        "        self._initial_state = initial_state\n",
        "        self.n_states = len(transition_probs)\n",
        "        self.reset()\n",
        "        self.np_random, _ = seeding.np_random(seed)\n",
        "\n",
        "    def get_all_states(self):\n",
        "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
        "        return tuple(self._transition_probs.keys())\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
        "        return tuple(self._transition_probs.get(state, {}).keys())\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
        "        return len(self.get_possible_actions(state)) == 0\n",
        "\n",
        "    def get_next_states(self, state, action):\n",
        "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
        "        assert action in self.get_possible_actions(\n",
        "            state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._transition_probs[state][action]\n",
        "\n",
        "    def get_transition_prob(self, state, action, next_state):\n",
        "        \"\"\" return P(next_state | state, action) \"\"\"\n",
        "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
        "        assert action in self.get_possible_actions(\n",
        "            state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n",
        "                                                                0.0)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" reset the game, return the initial state\"\"\"\n",
        "        if self._initial_state is None:\n",
        "            self._current_state = self.np_random.choice(\n",
        "                tuple(self._transition_probs.keys()))\n",
        "        elif self._initial_state in self._transition_probs:\n",
        "            self._current_state = self._initial_state\n",
        "        elif callable(self._initial_state):\n",
        "            self._current_state = self._initial_state()\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"initial state %s should be either a state or a function() -> state\" %\n",
        "                self._initial_state)\n",
        "        return self._current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
        "        possible_states, probs = zip(\n",
        "            *self.get_next_states(self._current_state, action).items())\n",
        "        next_state = possible_states[self.np_random.choice(\n",
        "            np.arange(len(possible_states)), p=probs)]\n",
        "        reward = self.get_reward(self._current_state, action, next_state)\n",
        "        is_done = self.is_terminal(next_state)\n",
        "        self._current_state = next_state\n",
        "        return next_state, reward, is_done, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(\"Currently at %s\" % self._current_state)\n",
        "\n",
        "    def _check_param_consistency(self, transition_probs, rewards):\n",
        "        for state in transition_probs:\n",
        "            assert isinstance(transition_probs[state],\n",
        "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
        "                                     \"but is instead %s\" % (\n",
        "                                         state, type(transition_probs[state]))\n",
        "            for action in transition_probs[state]:\n",
        "                assert isinstance(transition_probs[state][action],\n",
        "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
        "                                         \"a dictionary but is instead %s\" % (\n",
        "                                             state, action,\n",
        "                                             type(transition_probs[\n",
        "                                                 state, action]))\n",
        "                next_state_probs = transition_probs[state][action]\n",
        "                assert len(\n",
        "                    next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (\n",
        "                    state, action)\n",
        "                sum_probs = sum(next_state_probs.values())\n",
        "                assert abs(\n",
        "                    sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
        "                                             \"add up to %f (should be 1)\" % (\n",
        "                                                 state, action, sum_probs)\n",
        "        for state in rewards:\n",
        "            assert isinstance(rewards[state],\n",
        "                              dict), \"rewards for %s should be a dictionary \" \\\n",
        "                                     \"but is instead %s\" % (\n",
        "                                         state, type(transition_probs[state]))\n",
        "            for action in rewards[state]:\n",
        "                assert isinstance(rewards[state][action],\n",
        "                                  dict), \"rewards for %s, %s should be a \" \\\n",
        "                                         \"a dictionary but is instead %s\" % (\n",
        "                                             state, action, type(\n",
        "                                                 transition_probs[\n",
        "                                                     state, action]))\n",
        "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
        "              \" you will be sent at the first sign of defiance. \"\n",
        "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
        "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
        "\n",
        "\n",
        "def plot_graph(mdp, graph_size='10,10', s_node_size='1,5',\n",
        "               a_node_size='0,5', rankdir='LR', ):\n",
        "    \"\"\"\n",
        "    Function for pretty drawing MDP graph with graphviz library.\n",
        "    Requirements:\n",
        "    graphviz : https://www.graphviz.org/\n",
        "    for ubuntu users: sudo apt-get install graphviz\n",
        "    python library for graphviz\n",
        "    for pip users: pip install graphviz\n",
        "    :param mdp:\n",
        "    :param graph_size: size of graph plot\n",
        "    :param s_node_size: size of state nodes\n",
        "    :param a_node_size: size of action nodes\n",
        "    :param rankdir: order for drawing\n",
        "    :return: dot object\n",
        "    \"\"\"\n",
        "    s_node_attrs = {'shape': 'doublecircle',\n",
        "                    'color': '#85ff75',\n",
        "                    'style': 'filled',\n",
        "                    'width': str(s_node_size),\n",
        "                    'height': str(s_node_size),\n",
        "                    'fontname': 'Arial',\n",
        "                    'fontsize': '24'}\n",
        "\n",
        "    a_node_attrs = {'shape': 'circle',\n",
        "                    'color': 'lightpink',\n",
        "                    'style': 'filled',\n",
        "                    'width': str(a_node_size),\n",
        "                    'height': str(a_node_size),\n",
        "                    'fontname': 'Arial',\n",
        "                    'fontsize': '20'}\n",
        "\n",
        "    s_a_edge_attrs = {'style': 'bold',\n",
        "                      'color': 'red',\n",
        "                      'ratio': 'auto'}\n",
        "\n",
        "    a_s_edge_attrs = {'style': 'dashed',\n",
        "                      'color': 'blue',\n",
        "                      'ratio': 'auto',\n",
        "                      'fontname': 'Arial',\n",
        "                      'fontsize': '16'}\n",
        "\n",
        "    graph = Digraph(name='MDP')\n",
        "    graph.attr(rankdir=rankdir, size=graph_size)\n",
        "    for state_node in mdp._transition_probs:\n",
        "        graph.node(state_node, **s_node_attrs)\n",
        "\n",
        "        for posible_action in mdp.get_possible_actions(state_node):\n",
        "            action_node = state_node + \"-\" + posible_action\n",
        "            graph.node(action_node,\n",
        "                       label=str(posible_action),\n",
        "                       **a_node_attrs)\n",
        "            graph.edge(state_node, state_node + \"-\" +\n",
        "                       posible_action, **s_a_edge_attrs)\n",
        "\n",
        "            for posible_next_state in mdp.get_next_states(state_node,\n",
        "                                                          posible_action):\n",
        "                probability = mdp.get_transition_prob(\n",
        "                    state_node, posible_action, posible_next_state)\n",
        "                reward = mdp.get_reward(\n",
        "                    state_node, posible_action, posible_next_state)\n",
        "\n",
        "                if reward != 0:\n",
        "                    label_a_s_edge = 'p = ' + str(probability) + \\\n",
        "                                     '  ' + 'reward =' + str(reward)\n",
        "                else:\n",
        "                    label_a_s_edge = 'p = ' + str(probability)\n",
        "\n",
        "                graph.edge(action_node, posible_next_state,\n",
        "                           label=label_a_s_edge, **a_s_edge_attrs)\n",
        "    return graph\n",
        "\n",
        "\n",
        "def plot_graph_with_state_values(mdp, state_values):\n",
        "    \"\"\" Plot graph with state values\"\"\"\n",
        "    graph = plot_graph(mdp)\n",
        "    for state_node in mdp._transition_probs:\n",
        "        value = state_values[state_node]\n",
        "        graph.node(state_node,\n",
        "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
        "    return graph\n",
        "\n",
        "\n",
        "def get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n",
        "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return None\n",
        "    next_actions = mdp.get_possible_actions(state)\n",
        "    try:\n",
        "        q_values = [get_action_value(mdp, state_values, state, action, gamma) for\n",
        "                    action in next_actions]\n",
        "        optimal_action = next_actions[np.argmax(q_values)]\n",
        "    except NameError:\n",
        "        raise NameError(\"Implement and run the cell that has the get_action_value function\")\n",
        "        \n",
        "    return optimal_action\n",
        "\n",
        "\n",
        "def plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n",
        "    \"\"\" Plot graph with state values and \"\"\"\n",
        "    graph = plot_graph(mdp)\n",
        "    opt_s_a_edge_attrs = {'style': 'bold',\n",
        "                          'color': 'green',\n",
        "                          'ratio': 'auto',\n",
        "                          'penwidth': '6'}\n",
        "\n",
        "    for state_node in mdp._transition_probs:\n",
        "        value = state_values[state_node]\n",
        "        graph.node(state_node,\n",
        "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
        "        for action in mdp.get_possible_actions(state_node):\n",
        "            if action == get_optimal_action_for_plot(mdp,\n",
        "                                                     state_values,\n",
        "                                                     state_node,\n",
        "                                                     gamma):\n",
        "                graph.edge(state_node, state_node + \"-\" + action,\n",
        "                           **opt_s_a_edge_attrs)\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW5awxJjfI8f",
        "colab_type": "text"
      },
      "source": [
        "Let's define a simple Student MDP as shown in the image below. It has 5 states (green circles), 5 types of actions (red text on the arrows), and rewards for each action (blue text). \n",
        "\n",
        "For example, if we're at `Class 1` and we pick the `Study` action, we get a reward of `-2` (as we expend some energy and become tired). \n",
        "\n",
        "When we're at Class 3, we can choose to `Go Clubbing` and get a reward of `+1` (as it allows us to destress and have fun). However, you may end up forgetting what you learnt in `Class 1` and `Class 2`. So, there is a probability of `0.2` and `0.4` that you will go back to those respective states.\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://www.comp.nus.edu.sg/~neamul/Images/CS3244_1910/Student%20MDP.png\"  width=\"600\">\n",
        "</div>\n",
        "\n",
        "_(Diagram credit: Amrut Prabhu, NUS; CC BY 4.0)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcifszQQUREa",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 1):** What type of environment is the Student MDP?\n",
        "\n",
        "_Choose from: Stochastic; Deterministic_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECnjxWoIC3oP",
        "colab_type": "text"
      },
      "source": [
        "The next code block sets up the Student MDP represented by the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:44:32.642838Z",
          "start_time": "2018-04-02T13:44:32.545142Z"
        },
        "id": "k4i05OrofI8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Student MDP\n",
        "\n",
        "States: \n",
        "  - Class1\n",
        "  - Class2\n",
        "  - Class3\n",
        "  - Instagram \n",
        "  - Bed\n",
        "  \n",
        "Actions: \n",
        "  - Study\n",
        "  - Browse\n",
        "  - Quit\n",
        "  - Go Clubbing\n",
        "  - Sleep\n",
        "\"\"\"\n",
        "\n",
        "# Define the state transition probabilities, as shown in the diagram above\n",
        "transition_probs = {\n",
        "    'Class1': {\n",
        "        'Study': {'Class2': 1},\n",
        "        'Browse': {'Instagram': 1}\n",
        "    },\n",
        "    'Class2': {\n",
        "        'Study': {'Class3': 1},\n",
        "        'Sleep': {'Bed': 1},\n",
        "    },\n",
        "    'Class3': {\n",
        "        'Study': {'Bed': 1},\n",
        "        'Go Clubbing': {'Class1': 0.2, 'Class2': 0.4, 'Class3': 0.4},\n",
        "    },\n",
        "    'Instagram': {\n",
        "        'Browse': {'Instagram': 1},\n",
        "        'Quit': {'Class1': 1},\n",
        "    },\n",
        "    'Bed': {\n",
        "        'Sleep': {'Bed': 1},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the rewards, as shown in the diagram above\n",
        "rewards = {\n",
        "    'Class1': {\n",
        "        'Study': {'Class2': -2},\n",
        "        'Browse': {'Instagram': -1},\n",
        "    },\n",
        "    'Class2': {\n",
        "        'Study': {'Class3': -2},\n",
        "        'Sleep': {'Bed': 0},\n",
        "    },\n",
        "    'Class3': {\n",
        "        'Study': {'Bed': +10},\n",
        "        'Go Clubbing': {'Class1': +1, 'Class2': +1, 'Class3': +1},\n",
        "    },\n",
        "    'Instagram': {\n",
        "        'Browse': {'Instagram': -1},\n",
        "        'Quit': {'Class1': 0},\n",
        "    },\n",
        "    'Bed': {\n",
        "        'Sleep': {'Bed': 0},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create the Student MDP\n",
        "mdp = MDP(transition_probs, rewards, initial_state='Class1', seed=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub6ILVTxfI8n",
        "colab_type": "text"
      },
      "source": [
        "We can now use `mdp` just like any other gym environment. It also provides other methods that are needed for implementing the value iteration algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:44:34.203384Z",
          "start_time": "2018-04-02T13:44:34.199297Z"
        },
        "id": "wbdi0CJvfI8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('initial state =', mdp.reset())\n",
        "next_state, reward, done, info = mdp.step('Study') # Choose action `Study`\n",
        "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))\n",
        "\n",
        "#methods that are needed for implementing value iteration \n",
        "\n",
        "print(\"mdp.get_all_states() =\", mdp.get_all_states())\n",
        "print(\"mdp.get_possible_actions('Class1') =\", mdp.get_possible_actions('Class1'))\n",
        "print(\"mdp.get_reward('Class1', 'Study', 'Class2') =\", mdp.get_reward('Class1', 'Study', 'Class2'))\n",
        "print(\"mdp.get_next_states('Class3', 'Go Clubbing') =\", mdp.get_next_states('Class3', 'Go Clubbing'))\n",
        "print(\"mdp.get_transition_prob('Class3', 'Go Clubbing', 'Class2') =\", mdp.get_transition_prob('Class3', 'Go Clubbing', 'Class2'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvnFpuz-fI8x",
        "colab_type": "text"
      },
      "source": [
        "We can also visualize this MDP with the drawing functions like `plot_graph()` (defined in the hidden code block at the start of this section).\n",
        "\n",
        "The states are shown as green circles. Actions are represented as red circles. The red arrows show the choice of actions from a state. From a chosen action, the blue dotted arrows show the non-zero reward obtained and probability of transitioning to the next state. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:44:38.715883Z",
          "start_time": "2018-04-02T13:44:38.648684Z"
        },
        "id": "un7PLWKLfI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display\n",
        "        \n",
        "display(plot_graph(mdp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCl0uWP4fI82",
        "colab_type": "text"
      },
      "source": [
        "### b. Value Iteration\n",
        "\n",
        "Now let's build something to find the optimal policy for this MDP. One way of doing so is to find the optimal state-value function ($V^{*}$). This can be done using a simple iterative algorithm called **Value Iteration**. It converges to the correct $V^{*}$ values.\n",
        "\n",
        "The idea behind the algorithm is to begin with a random value function,and then use the Bellman optimality equation to obtain a better value function at each iteration, until convergence. The resultant value function is used to calculate the optimal policy.\n",
        "\n",
        "Let us define the terms used:\n",
        "- $v_{i}(s)$: the state-value function for state $s$ at step $i$. It is equal to the expected return starting from state $s$, and then following policy $\\pi$\n",
        "- $\\gamma$: discount factor (to give a higher weight to nearer rewards received than rewards received further in the future)\n",
        "- $P^{a}_{ss'}$: State transition probability of reaching state $s'$ if we take action $a$ at state $s$\n",
        "- $R^{a}_{s}$: Reward function that returns the reward when we take action $a$ at state $s$\n",
        "- $q_{i}(s,a)$: the action-value function for state $s$ and action $a$ at step $i$. It is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdw8Zgx7Y18h",
        "colab_type": "text"
      },
      "source": [
        "Here's the pseudocode for Value Iteration:\n",
        "\n",
        "---\n",
        "$\n",
        "\\text{Initialize } v_{(0)}(s) \\text{ to arbitraty values for all } s\\\\\n",
        "\\textbf{for } i=0,1,2... \\text{ do:} \\\\\n",
        "\\hspace{10mm} \\textbf{for } \\text{all } s \\in \\mathcal{S} \\text{ do:} \\\\\n",
        "\\hspace{20mm} \\textbf{for } \\text{all } a \\in \\mathcal{A} \\text{ do:} \\\\\n",
        "\\hspace{30mm} q_i(s, a) = \\sum_{s'} P^{a}_{ss'} \\cdot [ R^{a}_{s} + \\gamma v_{i}(s')] \\\\\n",
        "\\hspace{20mm} V_{(i+1)}(s) = \\max_a q_i(s,a) \\\\\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "From the above pseudocode, it is not obvious when to stop the Value Iteration algorithm. One stopping criteria that we can use is to check whether the maximum difference between two successive value functions is close enough, i.e., less than a threshold $\\epsilon$. The smaller the threshold is, the higher is the precision of the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h53xZ-5sXVXp"
      },
      "source": [
        "**Your Turn (Question 2):** Which of the following is true for the discount factor $\\gamma$?\n",
        "\n",
        "_Choose from: It prevents the total reward from going to 0; It prevents the total reward from going to infinity; The range of possible values is [0, 1]; The range of possible values is ($-\\infty, \\infty$)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PJ7unKBfI82",
        "colab_type": "text"
      },
      "source": [
        "First, let's write a function to compute the action-value function $q^{\\pi}$:\n",
        "\n",
        "$$q_i(s, a) = \\sum_{s'} P^{a}_{ss'} \\cdot [ R^{a}_{s} + \\gamma v_{i}(s')]$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCURRVjDHF0",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 3):** Complete the code below to compute the action-value function $q_i(s, a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:43:17.101416Z",
          "start_time": "2018-04-02T13:43:17.095468Z"
        },
        "id": "boA2GgMIfI83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action_value(mdp, state_values, state, action, gamma):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      mdp (MDP) : the MDP that we're using\n",
        "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
        "      state (string) : start state\n",
        "      action (string) : action that is taken \n",
        "      gamma (float) : discount factor\n",
        "\n",
        "    Returns:\n",
        "      q (float) : Returns action-value: Expected return starting from state s, \n",
        "                  taking action a, and then following policy π\n",
        "    \"\"\"\n",
        "    \n",
        "    q = 0\n",
        "    ######################################################\n",
        "    # Your Turn (Q3): write your own code here\n",
        "    #\n",
        "    # Compute q(s,a) as in formula above\n",
        "    #\n",
        "    # Do not change parameter state_values!\n",
        "    #\n",
        "    ######################################################\n",
        "\n",
        "    return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGz-xHDXfI88",
        "colab_type": "text"
      },
      "source": [
        "Using $q(s,a)$ we can now define the next $v(s)$ for value iteration.\n",
        " $$v_{(i+1)}(s) = \\max_a \\sum_{s'} P^{a}_{ss'} \\cdot [ R^{a}_{s} + \\gamma v_{i}(s')] = \\max_a q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XZQEU2tqiUQP"
      },
      "source": [
        "**Your Turn (Question 4):** Complete the code below to compute the state-value function for the next iteration, $v_{(i+1)}(s)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:43:17.103358Z",
          "start_time": "2018-04-02T13:43:05.506Z"
        },
        "id": "tqmVzwz4fI8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      mdp (MDP) : the MDP that we're using\n",
        "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
        "      state (string) : start state\n",
        "      gamma (float) : discount factor\n",
        "\n",
        "    Returns:\n",
        "      v (float) : Returns next state value: Expected return for next iteration, \n",
        "                  starting from state s, and then following policy π\n",
        "    \"\"\"\n",
        "\n",
        "    if mdp.is_terminal(state): return 0\n",
        "    \n",
        "    v = 0\n",
        "    ######################################################\n",
        "    # Your Turn (Q4): write your own code here\n",
        "    #\n",
        "    # Compute next v(s) as per formula above. \n",
        "    # Use the get_action_value() function (defined above) to get q(s,a).\n",
        "    #\n",
        "    # Do not change parameter state_values!\n",
        "    #\n",
        "    ######################################################\n",
        "\n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHlDGUxAfI9D",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Finally, let's combine everything we wrote into a working Value Iteration algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DWNBS4XlDD6"
      },
      "source": [
        "**Your Turn (Question 5):** Complete the code below to complete the Value Iteration algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5eC2ednrC2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(mdp, gamma = 0.9, num_iter = 1000, min_difference = 1e-5, state_values=None):\n",
        "    \"\"\"\n",
        "    Performs num_iter value iteration steps starting from state_values.\n",
        "\n",
        "    Args:\n",
        "      mdp (MDP) : the MDP that we're using\n",
        "      gamma (float) : discount factor for MDP\n",
        "      num_iter (string) : maximum number of iterations\n",
        "      min_difference (float) : stop Value Iteration if new values are at least this close to old values\n",
        "      state_values (dict- {string->float}) : state-values for the states\n",
        "\n",
        "    Returns:\n",
        "      state_values (dict) : Returns the state-values at the end of Value Iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize V(s) to 0 if argument is not provided\n",
        "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "    # Visualise MDP graph with initial state values\n",
        "    print(\"Initial MDP state values:\")\n",
        "    display(plot_graph_with_state_values(mdp, state_values))\n",
        "\n",
        "    new_state_values = {}\n",
        "    \n",
        "    for i in range(num_iter):\n",
        "        ######################################################\n",
        "        # Your Turn (Q5): write your own code here\n",
        "        #\n",
        "        # Compute new state values using the functions defined above.\n",
        "        # new_state_values is a dict {state : new_v(state)}\n",
        "        #\n",
        "        ######################################################\n",
        "\n",
        "        # Compute difference\n",
        "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
        "        \n",
        "        print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
        "        print(\"  \".join(\"V(%s) = %.3f\" % (s, v) for s,v in state_values.items()), end='\\n\\n')\n",
        "        \n",
        "        state_values = dict(new_state_values)\n",
        "\n",
        "        # Stopping criteria\n",
        "        if diff < min_difference:\n",
        "            print(\"Terminated\")\n",
        "            break\n",
        "            \n",
        "    return state_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOvSzdvu0kgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdp.reset()\n",
        "gamma = 1\n",
        "\n",
        "state_values = value_iteration(mdp, gamma, 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw_gjcHLfI9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Final MDP state values after running Value Iteration:\", state_values)\n",
        "display(plot_graph_with_state_values(mdp, state_values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAm3WzX7fI9K",
        "colab_type": "text"
      },
      "source": [
        "The Value Iteration algorithm has given us the optimal state-value function $v_{*}(s)$ (`state_values`, in our case). Now let's use this $v_{*}(s)$ to find the optimal actions in each state:\n",
        "\n",
        " $$\\pi_*(s) = argmax_a \\sum_{s'} P^{a}_{ss'} \\cdot [ R^{a}_{s} + \\gamma v_{i}(s')] = argmax_a q_i(s,a)$$\n",
        " \n",
        "The only difference here as compared to $v(s)$ is that we take the **argmax** and  not max, i.e., find the action that gives maximum $q(s,a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:43:17.107338Z",
          "start_time": "2018-04-02T13:43:05.525Z"
        },
        "id": "6wuLHqb9fI9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimal_action(mdp, state_values, state, gamma=1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      mdp (MDP) : the MDP that we're using\n",
        "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
        "      state (string) : start state\n",
        "      gamma (float) : discount factor\n",
        "\n",
        "    Returns:\n",
        "      optimal_action (string) : Returns the optimal action - action that results \n",
        "                                in the maximum action-value.\n",
        "    \"\"\"\n",
        "\n",
        "    if mdp.is_terminal(state): \n",
        "        return None\n",
        "    \n",
        "    actions = mdp.get_possible_actions(state)\n",
        "    \n",
        "    # Compute optimal action as per formula above. \n",
        "    optimal_action = None\n",
        "    optimal_action_value = - float(\"inf\")\n",
        "    for action in actions:\n",
        "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
        "\n",
        "        if action_value >= optimal_action_value:\n",
        "            optimal_action_value = action_value\n",
        "            optimal_action = action\n",
        "\n",
        "    return optimal_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:43:17.108149Z",
          "start_time": "2018-04-02T13:43:05.530Z"
        },
        "id": "A1B-KoU1fI9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Optimal action in state 'Class1':\", get_optimal_action(mdp, state_values, 'Class1', gamma))\n",
        "print(\"Optimal action in state 'Instagram':\", get_optimal_action(mdp, state_values, 'Instagram', gamma))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNCUqPo3nVFy",
        "colab_type": "text"
      },
      "source": [
        "We can also see these results in the MDP graph by using the `plot_graph_optimal_strategy_and_state_values()` function. The optimal actions for each state are denoted by the green arrows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:44:05.017823Z",
          "start_time": "2018-04-02T13:44:04.962755Z"
        },
        "id": "2Sr0iVmWfI9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(plot_graph_optimal_strategy_and_state_values(mdp, state_values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sSlmBqKn5Ma",
        "colab_type": "text"
      },
      "source": [
        "### .c Average reward\n",
        "\n",
        "Finally, let's calculate the average reward, $\\rho^{\\pi}$, received by our policy in each step. This can simply be calculated as the sum of the total immediate rewards earned divided by the number of transitions, calculated over a very long period of time.  For ergodic MDPs, $\\rho^{\\pi}$ is independent of the start state.\n",
        "\n",
        "Hence: \n",
        "$$\\rho^{\\pi} = \\lim\\limits_{T\\to \\infty} \\frac{1}{T} \\mathbb{E} \\left[ \\sum_{t=1}^{T} R_t \\right]$$\n",
        "\n",
        "**Note:** An MDP is _ergodic_ if the Markov chain induced by any policy is ergodic. \n",
        "A Markov chain is called an ergodic chain if it is possible to go from each state to every other state (not necessarily in one move).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-02T13:43:17.110002Z",
          "start_time": "2018-04-02T13:43:05.538Z"
        },
        "id": "DnoBM04QfI9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Measure agent's average reward\n",
        "s = mdp.reset()\n",
        "T = 100\n",
        "\n",
        "rewards = []\n",
        "for i in range(T):\n",
        "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "    rewards.append(r)\n",
        "\n",
        "print(\"Average reward: \", np.mean(rewards))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyjKPjkiLr-5",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 6):** How does the average reward for this exercise vary with the number of steps (T)? Choose all that apply.\n",
        "\n",
        "_Choose from: It decreases as T increases because our policy is not optimal;  It decreases as T increases because the Student MDP is not ergodic; It increases as T increases because our policy is not optimal; It increases as T increases because the Student MDP is ergodic_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtMmCHCTcVeA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kGaO2wXmyVP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Authored by Amrut Prabhu, Alvin Yip and [Min-Yen Kan](http://www.comp.nus.edu.sg/~kanmy) (2019), affiliated with [WING](http://wing.comp.nus.edu.sg), [NUS School of Computing](http://www.comp.nus.edu.sg) and [ALSET](http://www.nus.edu.sg/alset).\n",
        "Licensed as: [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/ ) (CC BY 4.0).\n",
        "Adapted from notebooks by [Practical Reinforcement Learning\n",
        "](https://www.coursera.org/learn/practical-rl/) on Coursera and [CS294-112](https://github.com/berkeleydeeprlcourse/homework/) from UC Berkeley.\n",
        "Please retain and add to this credits cell if using this material as a whole or in part.   Credits for photos given in their captions."
      ]
    }
  ]
}