{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08 » Neural Networks » CS3244 Machine Learning",
      "provenance": [],
      "collapsed_sections": [
        "2uK2c-52GKAf",
        "GSwt0q7LGOM5",
        "rhXwKzkiGpMb",
        "DfmMqD3uIHOd",
        "wkOpLZqZIqKU",
        "hOCQsBtwJDhO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nb19NV1uw2R",
        "colab_type": "text"
      },
      "source": [
        "Available at http://www.comp.nus.edu.sg/~cs3244/1910/08.colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvf81x6Vrysn",
        "colab_type": "text"
      },
      "source": [
        "![Machine Learning](https://www.comp.nus.edu.sg/~cs3244/1910/img/banner-1910.png)\n",
        "---\n",
        "See **Credits** below for acknowledgements and rights.  For NUS class credit, you'll need to do the corresponding _Assessment_ in [CS3244 in Coursemology](http://coursemology.org/courses/1677) by the respective deadline (as in Coursemology). \n",
        "\n",
        "**You must acknowledge that your submitted Assessment is your independent work, see questions in the Assessment at the end.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUAaknxMYo4I",
        "colab_type": "text"
      },
      "source": [
        "**Learning Outcomes for this Week** \n",
        "\n",
        "After watching the videos and completing the exercises for this week, you should be able to:\n",
        "\n",
        "*   Describe the basic idea of Artificial Neural Networks (shortened as ANN or NN):\n",
        "  * Explain how neural networks combine linear units with a non-linear activation function;\n",
        "  * Explain the topology of a NN;\n",
        "  * Given the index of a weight, find it in a graphical representation of a NN (and vice versa).\n",
        "* Name potential problems with NNs;\n",
        "* Describe how a neural network is trained using the backpropagation algorithm;\n",
        "* Name NN regularization techniques;\n",
        "* Obtain practical experience constructing NNs using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z3MeVMmS857",
        "colab_type": "text"
      },
      "source": [
        "_Welcome to the Week 08 Python notebook._ This week we will learn about **Neural Networks**.  We introduce **Neural Networks** in the lecture videos, and will be reviewing this material in the sixth tutorial.\n",
        "\n",
        "\n",
        "In this notebook, we will go through Neural Networks. We will be introducing `PyTorch` to work with neural network in this week and coming week's exercises. We will explore a programming exercise to build and train a neural network. In the post-tutorial section, we will go through _Backpropagation Algorithm_ with an example and learn some techniques to improve performance of our neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy5QuHHir65p",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Week 08: Pre-tutorial Work\n",
        "\n",
        "* Watch the CS 3244 video playlist for Week 08 Pre.  This will introduce the main concept for this week's class: _Neural Network_. \n",
        "* After watching the videos, complete the pre-tutorial exercises and questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxXcd0-CMHaz",
        "colab_type": "text"
      },
      "source": [
        "## 1 Basic questions from the videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcmgoJDBzL7J",
        "colab_type": "text"
      },
      "source": [
        "**NN Topology**:\n",
        "For Neural Networks, we will be using the following notation :\n",
        "* We indicate the layer number in superscript with `[]`. For example, $a^{[2]}$ indicates node $a$ of layer $2$.\n",
        "* $\\Theta_{ij}^{[l]}$ indicates weight of edge from $j$ node of layer $l - 1$ to node $i$ of layer $l$. \n",
        "\n",
        "\n",
        "You can find the description of standard notation used here, in the *Materials* folder of Coursemology. ([Download link](https://coursemology.org/courses/1677/materials/folders/38096/files/59020))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4C1wpEfMQG2",
        "colab_type": "text"
      },
      "source": [
        "The following are some simple questions you should be able to answer after watching the pre-videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfaNOUbrwwcB",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 1):** Which of the following problems might occur with Neural Networks?\n",
        "\n",
        "_Choose from: Difficult to optimize, Cannot learn complicated functions, Only usable for classification, Generalize badly, Inflexible_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDQF3mqKrJV",
        "colab_type": "text"
      },
      "source": [
        "![Here should be the picture of a neural network](https://www.comp.nus.edu.sg/~mstrobel/pict/NetworkForQuestion2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Ycuxz1yJ1w",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 2):** In the neural network above, what's the color of the weight $\\Theta^{[1]}_{12}$, as defined above?\n",
        "\n",
        "_Choose from: Violet, Blue, Purple, Black, Brown, Green_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfvhkT2vy1CL",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 3):** In the neural network above, identify the black weight (the arrow marked with black).\n",
        "\n",
        "_Choose from:  $\\Theta^{[1]}_{23}$, $\\Theta^{[1]}_{32}$, $\\Theta^{[2]}_{32}$, $\\Theta^{[2]}_{23}$,  $\\Theta^{[3]}_{23}$, $\\Theta^{[3]}_{32}$_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjH1mSkUL94i",
        "colab_type": "text"
      },
      "source": [
        "## 2 The XNOR Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PseOhHqUMppm",
        "colab_type": "text"
      },
      "source": [
        "In Week 3, we learned that a single perceptron cannot correctly model an XOR Gate (due to its underpowered model representation – a sort of underfitting or bias error). However, one way to solve the problem is to use a combination of two perceptrons. \n",
        "\n",
        "Ann heard about this combination idea.  In her work, she needs to implement a related function, the [XNOR](https://en.wikipedia.org/wiki/XNOR_gate) function. You can find its truth table below.\n",
        "\n",
        "| x1  | x2     | A XNOR B |\n",
        "|- - -  |- - -  | : - - - : |\n",
        "| -1  | -1  |  1               |\n",
        "| -1  | 1  |  -1               |\n",
        "| 1  | -1  |  -1               |\n",
        "| 1  | 1  |  1               |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgOoyXtfEgNB",
        "colab_type": "text"
      },
      "source": [
        "Unfortunately, she got a little distracted from creating a network to solve the problem. Instead, she focused on writing a function that can visualize a network, when given the *layer sizes* and *weights* of a network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2HA2LRnjy1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a library to plot graphs. We use it to visulize the networks.\n",
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "!pip install graphviz\n",
        "\n",
        "# Then let's do the normal imports, including the above library\n",
        "import numpy as np\n",
        "from graphviz import Digraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYjy4opKwybF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drawAFeedForwardNetwork(layers, weights, networkName=\"Network\"):\n",
        "  \"\"\"\n",
        "    Args:\n",
        "        layers (array of integers): array indicating # of nodes in each layer\n",
        "        weights (array of floats): The weight vector of our classifier\n",
        "        networkName (string): name of the network, default value \"Network\"\n",
        "\n",
        "    Returns:\n",
        "        network (Digraph): returns the generated graph\n",
        "        \n",
        "  \"\"\"\n",
        "  network = Digraph(name=networkName)\n",
        "  network.graph_attr['rankdir'] = 'LR'\n",
        "  layerNumber = 0\n",
        "  \n",
        "  # Adding all the nodes\n",
        "  for layer in layers:\n",
        "    # Iterate through each layer and add nodes\n",
        "    for node in range(layer):\n",
        "      # Defining the name of the node\n",
        "      if (layerNumber == 0):\n",
        "        nodeName = \"x{}\".format(node+1)\n",
        "      else:\n",
        "        nodeName = \"a\"\n",
        "      network.node(\"{}{}\".format(layerNumber,node),nodeName,fontname=\"Symbol\")\n",
        "      \n",
        "    # For all but the last layer, we add the bias term\n",
        "    if(layerNumber+1 != len(layers) ):\n",
        "      network.node(\"{}{}\".format(layerNumber,\"b\"),\"1\")\n",
        "    layerNumber += 1;\n",
        "    \n",
        "  # Adding the edges\n",
        "  for layer in range(layerNumber-1):\n",
        "    for innode in range(layers[layer+1]):\n",
        "      for outnode in range(layers[layer]):\n",
        "        network.edge(\"{}{}\".format(layer,outnode),\"{}{}\".format(layer+1,innode),str(weights[layer][outnode][innode]))\n",
        "      network.edge(\"{}{}\".format(layer,\"b\"),\"{}{}\".format(layer+1,innode),str(weights[layer][layers[layer]][innode]))\n",
        "  \n",
        "  # Finally we add the output arrow\n",
        "  network.node(\"out\",\"h(x)\",shape='plaintext')\n",
        "  network.edge(\"{}{}\".format(layerNumber-1,0),\"out\")\n",
        "  \n",
        "  return network  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prN9wTBhA6zN",
        "colab_type": "text"
      },
      "source": [
        "Oops!  Ann ran out of time, and now she's turning to you, her ML teammate, for some help.  \n",
        "\n",
        "She already entered some of weights that she's certain can help construct the function but she's missing a few edges' weights. Can you help her finish the network? Note that the _activation function_, $g(x)$, in each node is $sign()$. \n",
        "The $sign()$ function is defined as follows \n",
        "$$ sign(z) = \\begin{cases} 1 &\\text{if }z> 0\\\\\n",
        "0 &\\text{if } z = 0\\\\\n",
        "-1 &\\text{if } z < 0  \n",
        "\\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r63-Ocmw0M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the network Ann created\n",
        "weights = np.asarray([[[-1,1],[1,\"Q4_1\"],[-1.5,-1.5]],[[-1],[-1],[\"Q4_2\"]]])\n",
        "drawAFeedForwardNetwork([2,2,1],weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXcbZc32GnQo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Your Turn (Question 4): What should the value of Q4_1 and Q4_2 be so that the whole network becomes an XNOR function, as defined in the table above? Put your answers here, as well as in the assessment.\n",
        "Q4_1 =  #@param {type:\"number\"}\n",
        "Q4_2 =  #@param {type:\"number\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Y7ks-ZGpdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Qweights = np.asarray([[[-1,1],[1,Q4_1],[-1.5,-1.5]],[[-1],[-1],[Q4_2]]])\n",
        "drawAFeedForwardNetwork([2,2,1],Qweights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYf_HkHqWV9d",
        "colab_type": "text"
      },
      "source": [
        "Suppose now, after creating some artificial datasets, Ann wants to train a neural network that outputs XNOR using the $sign()$ activation function in the hidden layers by backpropagation. She used the same neural network topology, but instead of using the weights above, she initialised the weights of her neural network randomly. After training several epochs, she realised that the weights of her neural network remained unchanged.    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iTQC52bCrdR",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 5):** What could be the problem with Ann's neural network?\n",
        "\n",
        "_Choose from:\n",
        "The gradient of `sign()` is $0$ everywhere, hence backpropagation would have resulted in no change of weights; The `sign()` function is discontinuous so she is unable to do backpropagation on a discontinuous (hence, non-differentiable) function; She did not collect enough data; She did not ensure that the weights were non-zero._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIJ_WnFTNU8V",
        "colab_type": "text"
      },
      "source": [
        "## 3 Programming : Introducing PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9JnCAxrYFH_",
        "colab_type": "text"
      },
      "source": [
        "So far in this course, we have relied mostly on the [scikit learn](http://scikit-learn.org/stable/index.html) library, which contains many implementations of ML algorithms. Unfortunately, its implementations of neural networks are rather limited.  Importantly, there is no graphical processing unit (GPU) support for general computation, so crucial to the training of larger networks.  Thus, we wouldn't want to use it for training NNs.\n",
        "\n",
        "So, for this part of the course, we will use PyTorch, one of the major open source neural network libraries. If you find that you don't like PyTorch, you can check out [Tensorflow](https://www.tensorflow.org/), [Keras](https://keras.io/) or [FastAI](http://www.fast.ai/) (or any other you might find), but we introduce PyTorch as it has particularly good modularization that is helpful for teaching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Qni8tRawey",
        "colab_type": "text"
      },
      "source": [
        "To get started we import pytorch packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugzl5Lk6aZ_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "from  sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uK2c-52GKAf",
        "colab_type": "text"
      },
      "source": [
        "### .a Create a dataset to work on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpG0_EnD1dhS",
        "colab_type": "text"
      },
      "source": [
        "Let's start by creating a data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGHLw-091pqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We first create some points in the 2D space\n",
        "x,y = sklearn.datasets.make_circles(n_samples=100, random_state=42,factor=0.5)\n",
        "\n",
        "# Here we split them in training and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.33, random_state=42)\n",
        "\n",
        "# And now we can plot them\n",
        "plt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap=\"bwr\")\n",
        "plt.scatter(X_val[:,0],X_val[:,1],c=y_val,cmap=\"bwr\",marker=\"s\")\n",
        "plt.title(\"A linearly non seperaple data set\")\n",
        "plt.legend([\"Train\",\"Validation\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSwt0q7LGOM5",
        "colab_type": "text"
      },
      "source": [
        "### .b Load the dataset in `DataLoader`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5DXiQt_5bek",
        "colab_type": "text"
      },
      "source": [
        "To be able to use the data set with  PyTorch we have to transform the numpy arrays to  **tensors** , which are basically the PyTorch version of an array (if you want to learn more about them see for example [here](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html)).\n",
        "\n",
        "Next, we create **Data loaders**. So far to iterate through our training data we just used a `for` loop. A more elegant way to do this is using a data loader, here you can easily configure if you want to shuffle your data, how many examples you one to use in each step (i.e. the batch size ) and many other things.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLkIbqP_3zjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we transform our numpy arrays to Tensors\n",
        "X_train_T, X_val_T, y_train_T, y_val_T = torch.from_numpy(X_train),torch.from_numpy(X_val), torch.from_numpy(y_train), torch.from_numpy(y_val)\n",
        "\n",
        "# Here we create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_T.float(),y_train_T.float().view(-1,1)), batch_size=100)\n",
        "\n",
        "validationloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_T.float(),y_val_T.float().view(-1,1)), batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhXwKzkiGpMb",
        "colab_type": "text"
      },
      "source": [
        "### .c Build a Simple NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrj1jBO-cvie",
        "colab_type": "text"
      },
      "source": [
        "Equiped with the data, we can now define a neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w70jzEUWcRyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = nn.Sequential(\n",
        "    #maps linearly, a vector of 2 dimension, with bias, into a 1 dimensional input \n",
        "    nn.Linear(in_features = 2, out_features = 1, bias = True),\n",
        "    #the single output is then fed in to the sigmoid activation function\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvj_Nw_NBGeE",
        "colab_type": "text"
      },
      "source": [
        "Here's another way that we could use to define the same neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyy-Y3ONA2GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define neural network by creating it as a subclass of nn.Module\n",
        "class Net(nn.Module):\n",
        "\n",
        "  #Object initialisation in python, can be customised\n",
        "  #to have other variables\n",
        "  def __init__(self, input_size = 2, output_size = 1):\n",
        "    #Creating the object net\n",
        "    super(Net, self).__init__()\n",
        "    #Maps linearly with bias, y = Wx + b\n",
        "    self.linear1 = nn.Linear(input_size, output_size, True)\n",
        "    #Sigmoid function\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  #Tells the neural net how to feedforward a input x\n",
        "  #this forward will also be subsequently tell the \n",
        "  #the module how to calculate the backpropagated gradients \n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    #Alternatively, can choose to do x = torch.Sigmoid(x) instead\n",
        "    #and we declare one fewer layer in __init__\n",
        "    x = self.sigmoid(x)\n",
        "    #returns the output we calculated\n",
        "    return x  \n",
        "\n",
        "net = Net(2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQRaCXRzfLXm",
        "colab_type": "text"
      },
      "source": [
        "Here is an explanation of the lines above:\n",
        "\n",
        "* [nn.Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)  is a sequential container that will create a network, out of the layers (or subnetworks) that are passed into it, in exactly the order they are passed in. It can be used to create simple networks. For example in the codeblock above, we map linearly, a vector of two features to 1 feature with bias, and thereafter, we feed the outputs to a sigmoid function. The diagram is a visualisation of how the neural network looks like in detail:\n",
        "\n",
        " ![Image of NN above](https://raw.githubusercontent.com/monikernemo/CS3244-Images/master/Neural%20Network.PNG)  \n",
        "But typically, one would lump the activation function with the hidden layers and not separate it into two nodes. However, we show the linear output and activation function separately to illustrate what nn.Sequential tries to do.\n",
        "\n",
        "* [nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) is a basic linear layer $\\mathbf{y} =\\mathbf{W^\\top x}+\\mathbf{b}$ where our input is the $2$ dimensional vector, the weight matrix $\\mathbf{W}$ is a $2\\times1$ matrix, and the bias weights, $\\mathbf{b}$ is a $1$ dimensional vector. It doesn't really map linearly in the sense of linear algebra because of the bias, but it is really an affine linear transformation, so essentially, matrix multiplication and adding of constants. In general, suppose $\\mathbf{x}$ has input dimensions of $n$, then $\\mathbf{W}$ can be a $n\\times m$ matrix and $\\mathbf{b}$ a $m$ dimensional vector and $\\mathbf{y} =\\mathbf{W^\\top x}+\\mathbf{b}$ would be a $m$ dimensional vector as well.\n",
        "\n",
        "* [nn.Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid) this is the Sigmoid function, $\\sigma$ we have seen before  (in Week 4). But of course, since we are working with multiple inputs, this Sigmoid function can take in vector-values and gives out vector values. More concretely, suppose $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$ then we \"abuse notation\" and allow  $\\sigma(\\mathbf{x}) = (\\sigma({x_1}), \\sigma({x_2}), \\dots , \\sigma(x_n))$. Typically, this is what happens for almost all activation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfmMqD3uIHOd",
        "colab_type": "text"
      },
      "source": [
        "### .d Loss Function and Optimizer in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUT_gweiiStW",
        "colab_type": "text"
      },
      "source": [
        "PyTorch also provides predefined loss functions and optimizers. Here we use:\n",
        "\n",
        "* [nn.BCELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss) as a loss function. It measures the Binary Cross Entropy between the output of the network and the actual label. We actually have seen it before as the Log-Likelihood loss in Week 4.  \n",
        "To recap, suppose we have a binary classification problem, where $(\\mathbf{x}^{(j)}, y^{(j)})$ is a training pair with $y^{(j)} \\in \\{0, 1\\}$ and let $h: \\mathbb{R}^n \\rightarrow [0,1] $ be our classifier/hypothesis. The Binary Cross Entropy loss is the following loss function: $j(h, \\mathbf{x}_i) = -y^{(j)}\\log [h(\\mathbf{x}^{(j)})]  - (1-y^{(j)})[1 - \\log h(\\mathbf{x}^{(j)})]$. Intutively, when $y^{(j)} = 1$, we want $\\mathbf{x}^{(j)}$ to be close to 1 so it has high cost when $\\mathbf{x}^{(j)}$ is close to zero since we take $\\log h(\\mathbf{x}^{(j)})$. On the other hand if $y^{(j)} = 0 $ then we want $\\mathbf{x}^{(j)}$ close to $0$.\n",
        "\n",
        "* [optim.SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) as an optimizer, and as the name suggest this is *Stochastic Gradient Descent*. We have to link the weights of the network to the optimizer (i.e. net.parameters) and give it a learining rate, here we use 0.1. There are many other optimizers and more parameters to tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNG0vIFW6iZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Can have other loss function such as, Mean Square Error\n",
        "# but Binary Cross Entropy loss is more useful for binary \n",
        "# classification problem\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "# Params - refers to the neural network's parameter that we want to optimise \n",
        "# lr - refers to the learning rate we want to train \n",
        "opt = optim.SGD(params = net.parameters(), lr = 1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkOpLZqZIqKU",
        "colab_type": "text"
      },
      "source": [
        "### .e Train our NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4lmcT4GkFuG",
        "colab_type": "text"
      },
      "source": [
        "Now, we can write our `train` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr4sC5N76g-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, trainloader, validationloader, lossfunction, optimizer, n_epochs=100):\n",
        "  \"\"\"\n",
        "    Args:\n",
        "        model (pytorch neural network): the network we want to train\n",
        "        trainloader (data loader): The data loader for the training set\n",
        "        validationloader (data loader): The data loader for the validation set\n",
        "        lossfunction (a pytorch loss function): The loss function used to train the network\n",
        "        optimizer (a pytorch optimizer ): The optimizer used to train the network\n",
        "        n_epochs (int): The number of epochs the network is trained \n",
        "        (epoch is just the number of times the model backpropagates/trains on the entire data set)\n",
        "    Returns:\n",
        "        trainingLosses,validationLosses (Lists of floats): returns the generated graph\n",
        "        \n",
        "  \"\"\"\n",
        "  trainingLosses, validationLosses = [],[]\n",
        "  for t in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = Variable(inputs), Variable(labels).float() # See the comments below (1)\n",
        "\n",
        "      optimizer.zero_grad()   # See the comments below (2)\n",
        "\n",
        "      outputs = model(inputs) # See the comments below (3)\n",
        "      \n",
        "      # Compute the loss\n",
        "      loss = lossfunction(outputs, labels) \n",
        "      # Compute the gradient for each variable\n",
        "      loss.backward()  \n",
        "      #Update the weights according to the computed gradient in previous step\n",
        "      #Remember that optimiser already has the information of weights to be \n",
        "      #optimised in previous code block\n",
        "      optimizer.step() \n",
        "      \n",
        "      # for printing\n",
        "      running_loss += loss.data.item()\n",
        "      \n",
        "    # This second loop is actually just calculating the loss in the validation set\n",
        "    # Otherwise, it's the same as above\n",
        "    running_loss_val = 0.0\n",
        "    for i, data in enumerate(validationloader):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = Variable(inputs), Variable(labels).float()\n",
        "      outputs = model(inputs) \n",
        "      loss = lossfunction(outputs, labels) # Compute the loss\n",
        "      \n",
        "      # for printing\n",
        "      running_loss_val += loss.data.item()\n",
        "    trainingLosses.append(running_loss)\n",
        "    validationLosses.append(running_loss_val)\n",
        "    # you can set this for every epoch or every say, 5 samples\n",
        "    if t % 5 == 5-1:\n",
        "      print(\"Epoch: {} Training loss: {:f} Validation loss: {:f}\".format(t+1,running_loss,running_loss_val))\n",
        "  return trainingLosses,validationLosses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03QNR8cyiV1f",
        "colab_type": "text"
      },
      "source": [
        "**Notes on the train function**:\n",
        "\n",
        "1. `Variable(inputs)` here a `Tensor` is converted to a `Variable`. PyTorch automatically calculates the gradient towards `Variables`. A functionality you can read more about [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). Actually, this functionality got recently moved into `Tensors`, so in newer versions of PyTorch you don't need to make this call anymore. However, for the version we use in this notebook we still need it.\n",
        "2.   `optimizer.zero_grad()` sets the gradients of the parameters back to zero. In PyTorch the gradients are saved as an attribute of the parameters and if they are not set to zero after each update, they will accumulate an lead to arbitrary results.\n",
        "3. `outputs = model(inputs)`  this is actually one of the nicest features of PyTorch you can just call a network like a function and it maps an input to an output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiBsPt-BieEY",
        "colab_type": "text"
      },
      "source": [
        "**Let's continue** with our training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2GYcV9_898L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(net,trainloader,validationloader,loss,opt);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOCQsBtwJDhO",
        "colab_type": "text"
      },
      "source": [
        "### .f Visualize the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_wAEdN1_cXK",
        "colab_type": "text"
      },
      "source": [
        "Hm... the loss doesn't seem to go down much. Let's see maybe the classifier works good anyway. We have the following visualization of the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B5y4paLAl8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotResults(net,X_train,y_train,X_val,y_val,title=\"\"):\n",
        "  \"\"\"\n",
        "    Args:\n",
        "        net (pytorch neural network): The network we want to take a look at\n",
        "        X_train (numpy array): The training points\n",
        "        y_train (numpy array): The the training labels\n",
        "        X_val (numpy array): The validation points\n",
        "        y_val (numpy array): The validation labels\n",
        "        title (string): The title of out plot\n",
        "  \"\"\"\n",
        "  # Here we create a grid to see the value of our network on many points in the area\n",
        "  X = np.arange(X_train[:,0].min()*1.2,X_train[:,0].max()*1.2, 0.01)\n",
        "  Y = np.arange(X_train[:,1].min()*1.2,X_train[:,1].max()*1.2, 0.01)\n",
        "  X, Y = np.meshgrid(X, Y)\n",
        "  \n",
        "  # Now we evaluate the network\n",
        "  Z = np.zeros(X.shape).flatten()\n",
        "  for i,[x,y] in enumerate(zip(X.flatten(),Y.flatten())):\n",
        "    Z[i] = np.asarray(net(Variable(torch.Tensor([x,y])).float()).data)\n",
        "  Z = Z.reshape(X.shape)\n",
        "\n",
        "  # Standard figure definition\n",
        "  fig = plt.figure(figsize=[10,5])\n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.set_xlim(X_train[:,0].min()*1.1,X_train[:,0].max()*1.1)\n",
        "  ax.set_ylim(X_train[:,1].min()*1.1,X_train[:,1].max()*1.1)\n",
        "\n",
        "  # The next line plots the contour\n",
        "  ax.contourf(X, Y, Z,cmap='bwr',alpha=0.8)\n",
        "  plt.title(title)\n",
        "  plt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap=\"bwr\")\n",
        "  plt.scatter(X_val[:,0],X_val[:,1],c=y_val,cmap=\"bwr\",marker=\"s\")\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z75U7H3f-TeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotResults(net,X_train,y_train,X_val,y_val,title=\"Our first result\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9ipjeopc0z",
        "colab_type": "text"
      },
      "source": [
        "Okay, that doesn't look good at all. But that's not too surprising, our network structure is only one layer (we actually just did logistic regression from Week $03$), so we created a linear classifier. That cannot work. Let's try to create an actual network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqJAnVUuJbdE",
        "colab_type": "text"
      },
      "source": [
        "### .g Build your own NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR1uLZQauMTR",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 6)**: Define a network `net2` with one additional hidden layer, that can actually solve the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR3GvNxCpcK",
        "colab_type": "text"
      },
      "source": [
        "**Tips:**\n",
        "\n",
        "* If you just add a ```nn.Linear```, you will still end up with a linear classifier. You'll need to add a [nonlinear activation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). ```nn.ReLU()``` works fine, but you can try out others.\n",
        "\n",
        "* The network doesn't need to be complex, one extra layer and a couple of nodes should be enough.\n",
        "\n",
        "* Make sure that the output of the first linear layer and the input of the second linear layer have the same size.\n",
        "\n",
        "* Don't forget to redefine the ```optimizer```. It needs to be linked to the parameters of the network.\n",
        "\n",
        "* You might need to increase the number of epochs, to get a perfect result. Please don't go crazy with it as if you go beyond 10,000, you might just crash your computer.  You have been warned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHTHuHCpm6jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######\n",
        "# Write your code here, referencing the previous net block\n",
        "# Feel free to modify the network\n",
        "######\n",
        "net2 = nn.Sequential(\n",
        "    #maps linearly, a vector of 2 dimension, with bias, into a 1 dimensional input \n",
        "    nn.Linear(in_features = 2, out_features = 1, bias = True),\n",
        "    #the single output is then fed in to the sigmoid activation function\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "#######"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uutZkkkIGLX2",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn**: Now go ahead and train `net2` as we trained the first network above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPRZp9kgSFR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace with your loss function\n",
        "loss = None\n",
        "# Replace with your optimizer\n",
        "opt2 = None\n",
        "# May be 1 epoch is not enough!\n",
        "train(net2,trainloader,validationloader,loss,opt2, n_epochs= 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJIEn0mpVPGH",
        "colab_type": "text"
      },
      "source": [
        "You can visualize your result now and see if it is better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQND5BHpBlpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotResults(net2,X_train,y_train,X_val,y_val,title=\"Our improved result\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZX4JTdyIRuQ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Week 08: Post-Tutorial Work\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct61c8aPKxcv",
        "colab_type": "text"
      },
      "source": [
        "## 4 Back-propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI6CCIQQK0MR",
        "colab_type": "text"
      },
      "source": [
        "### .a A Toy Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQqLwNlvUOy0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "For most of the people, neural networks are like a black box, especially the Backpropagation algorithm. Backpropagation is a common method to train a neural network, and also one of the hardest parts for understanding neural networks. Fortunately, most of the deep learning packages nowadays can automatically perform backpropagation for you, which relieves you from complicated mathematical derivations. However, backpropagation is still an indispensable part to get an in-depth understanding of neural networks.\n",
        "\n",
        "I think you have already learned theoretically how backpropagation works in the post-class video. In this post-class notebook, we will lead you to go through a concrete toy example, to help you develop a more intuitive understanding of how backpropagation works. OK, Let's get started. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu4_I8TS4R_m",
        "colab_type": "text"
      },
      "source": [
        "For this toy example, we’re going to use a neural network with **a 2-d input, one hidden layer with two neurons and two output neurons.** Additionally, the hidden neurons and the input **will include a bias.**\n",
        "\n",
        "Here’s the basic structure:\n",
        " \n",
        " <div align=\"center\">\n",
        "<img src=\"https://comp.nus.edu.sg/~neamul/Images/nn_example.png\" width=400>\n",
        "<p> Figure 1. A Toy Neural Network.  \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWlXyjRf5_TS",
        "colab_type": "text"
      },
      "source": [
        "To make things easy enough, suppose we only have one input example $x = (2, 3)$, which means that $x_1 = 2$, and $x_2 = 3$. And suppose we have a two dimentional target $y$ for the input $x$, and $y = (0.1, 0.9)$. The loss function is defined as the **square of euclidean distance** between the network output $\\mathbf{a}^{[2]}  = (a^{[2]}_1, a^{[2]}_2)$ and the actual target $\\mathbf{y} = (y_1, y_2)$, as follows. \n",
        "\n",
        "$$\n",
        "J(\\mathbf{a}^{[2]},\\mathbf{y}) = \\| \\mathbf{a}^{[2]} - \\mathbf{y} \\|_2^2 = (a^{[2]}_1 - y_1)^2 + (a^{[2]}_2 - y_2)^2\n",
        "$$\n",
        "\n",
        "Moreover, we initialize the weights for the above network, as follows.\n",
        "\n",
        "$$\n",
        "\\Theta^{[1]} =  \n",
        "\\begin{bmatrix}\n",
        "- 0.1 & 0.3 \\\\\n",
        "0.2 & - 0.4\n",
        "\\end{bmatrix}, \n",
        "\\Theta^{[2]} =  \n",
        "\\begin{bmatrix}\n",
        "0.5 & 0.7 \\\\\n",
        "- 0.6 & - 0.8\n",
        "\\end{bmatrix}, \n",
        "\\mathbf{b}^{[1]} = (0.1, 0.1), \\mathbf{b}^{[2]} = (0.1, 0.1)\n",
        "$$\n",
        "\n",
        "where the $i$-th row and $j$-th column of $\\Theta^{[l]}$ corresponds to $\\Theta^{[l]}_{ij}$ and $\\mathbf{b}^{[l]}$ is the bias vector that goes into layer $l$. Suppose if $R$ is the desired activation function, we can calculate the output of a node  using the following equation:\n",
        "\n",
        "$$ \\mathbf{a}^{[2]} = R(\\Theta^{[2]\\top}\\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}) $$\n",
        "\n",
        "where $R$ is applied point-wise to the result. \n",
        "\n",
        "The goal of backpropagation is essentially to optimize the weights so that the neural network can learn how to correctly map arbitrary inputs to outputs. In this toy example where we only have one training example $(x,y)$, the problem essentially becomes: given inputs $x_1 = 2$, and $x_2 = 3$, we want the neural network to output $a^{[2]}_1 = 0.1$, and $a^{[2]}_2 = 0.9$, because in this case we will obtain a minimal loss $J(\\mathbf  a^{[2]}, \\mathbf y) = 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO8blS9bXEhH",
        "colab_type": "text"
      },
      "source": [
        "### .b Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds2XgEka_pXu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "We will work step-by-step to see how the parameters change after we perform backpropagation on this single training example.  But first, we need to run the forward propagation. Since this is easy, we want you to calculate yourself. \n",
        "\n",
        "Note that for hidden layer $(a^{[1]}_1, a^{[1]}_2)$, we use the **ReLU function** as the nonlinearity. The ReLU is the most used activation function in the world right now because, it is used in almost all the convolutional neural networks or deep learning. The formula of ReLU function is as follows. \n",
        "\n",
        "$$\n",
        "R(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "You can refer to the following figure to see the difference between Sigmoid and ReLU function. \n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/max/726/1*XxxiA0jJvPrHEJHD4z893g.png\" width=400>\n",
        "<p> Figure 2. The ReLU and Sigmoid Function.  </p>\n",
        "</div>\n",
        "\n",
        " (Diagram Credit: From [https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6](https://miro.medium.com/max/726/1*XxxiA0jJvPrHEJHD4z893g.png))\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qmo61wLtwBN",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn:** Try to calculate the following values after forward proapagation : $a^{[2]}_1$, $a^{[2]}_2$ and $J(\\mathbf a^{[2]}, \\mathbf y)$\n",
        "\n",
        "**N.B.** Try to do it on your own before coming to Tutorial $06$. We will discuss more about this during the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6wVoPfvXGWb",
        "colab_type": "text"
      },
      "source": [
        "### .c Backward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkFletr0GI9C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "After you obtained the loss $J(\\mathbf{a}^{[2]}, \\mathbf{y})$ after forward propagation, now you want to update the weights based on the loss. To do this, you have to calculate the gradient, that is the partial derivative of $J(\\mathbf{a}^{[2]}, \\mathbf{y})$ with respect to $\\Theta_{ij}^{[l]}$ and $\\mathbf{b}^{[1]}, \\mathbf{b}^{[2]}$ and then perform the update as follows. \n",
        "\n",
        "$$\n",
        "\\Theta_{ij}^{[2]} = \\Theta_{ij}^{[2]}- \\alpha \\frac{\\partial J(\\mathbf a^{[2]},\\mathbf y)}{\\partial \\Theta_{ij}^{[2]}} \\\\\n",
        "\\mathbf b_i^{[2]} = \\mathbf  b_i^{[2]} - \\alpha \\frac{\\partial J(\\mathbf  a^{[2]}, \\mathbf y)}{\\partial \\mathbf b^{[2]}_i} \n",
        "$$\n",
        "\n",
        "Here $\\alpha$ is the learning rate. To calculate the gradients, we need to use the chain rule. So the order of calculating should be backward, i.e. starting from the output layer ($a^{[2]}_1, a^{[2]}_2$), then goes to the hidden layer $\\Theta^{[2]}$ and ($a^{[1]}_1, a^{[1]}_2$), and finally the input layer $\\Theta^{[1]}$ and $(x_1, x_2)$. \n",
        "\n",
        "OK, let us first calculate the gradient with respect to $a^{[2]}_1$ and $a^{[2]}_2$. \n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf  a^{[2]},\\mathbf y)}{\\partial  a^{[2]}_1} = \\frac{\\partial }{\\partial a^{[2]}_1} \\{(a^{[2]}_1 - y_1)^2 + (a^{[2]}_2 - y_2)^2\\} = 2 \\times (a^{[2]}_1 - y_1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a^{[2]}_2} = \\frac{\\partial }{\\partial a^{[2]}_2} \\{(a^{[2]}_1 - y_1)^2 + (a^{[2]}_2 - y_2)^2\\} = 2 \\times (a^{[2]}_2 - y_2)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YRCzo2rJvYP",
        "colab_type": "text"
      },
      "source": [
        "Then, let us go backward and calculate the gradient of $\\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial \\Theta_{11}^{[2]}}$. We can rewrite it as following:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial \\Theta_{11}^{[2]}} = \\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a^{[2]}_1}\\times \\frac{\\partial a^{[2]}_1}{\\partial \\Theta_{11}^{[2]}}\n",
        "$$\n",
        "\n",
        "See? In order to calculate $\\Theta_{11}^{[2]}$, we use the previous results $\\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a^{[2]}_1}$, this can be regarded as the error signals $\\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a^{[2]}_1}$ flows back from $a^{[2]}_1$ to $\\Theta_{11}^{[2]}$, the error signal on $\\Theta_{11}^{[2]}$ is calculated based on its **local gradient** $\\frac{\\partial a^{[2]}_1}{\\partial \\Theta_{11}^{[2]}}$ and **the error signal passed down from higher layers**. This is the intuition of backpropagation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gel8VKxTiazt",
        "colab_type": "text"
      },
      "source": [
        "Now, it's time for you to do some calculations. To make your calculation easier, suppose we already know the following:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a^{[2]}_1} = 0.5, \\\\ \\frac{\\partial J(\\mathbf a^{[2]},\\mathbf y)}{\\partial a^{[2]}_2} = 0.3,\\\\ a^{[1]}_1 = 0.5, a^{[1]}_2 = 0.4\n",
        "$$\n",
        "\n",
        "**N.B.**: Note that these may not be the correct values for the above graph. We don't want your calculation to go wrong because of your previous errors. Instead, use the provided information here to calculate the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeB2kfEUu0YJ",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn:** Calculate the following gradient (partial derivative) \n",
        "\n",
        "1. $J(\\mathbf a^{[2]}, \\mathbf y)$ with respect to $\\Theta^{[2]}_{21}$.\n",
        "2. $J(\\mathbf a^{[2]}, \\mathbf y)$ with respect to $\\Theta^{[2]}_{12}$.\n",
        "\n",
        "**N.B.** Try to do it on your own before coming to Tutorial $06$. We will discuss more about this during the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgtcjuZtOgzB",
        "colab_type": "text"
      },
      "source": [
        "OK, now we have calculated the gradients for $\\Theta^{[1]}$, let us back propagate more to $a^{[1]}_1$ and $a^{[1]}_2$. The partial derivative of $J(\\mathbf a^{[2]}, \\mathbf y)$ with respect to $a^{[1]}_1$ is as follows. \n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a_1^{[1]}} = \\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a^{[2]}_1} * \\frac{\\partial a^{[2]}_1}{\\partial a^{[1]}_1} + \\frac{\\partial J(\\mathbf a^{[2]}, \\mathbf y)}{\\partial a^{[2]}_2} * \\frac{\\partial a^{[2]}_2}{\\partial a^{[1]}_1}\n",
        "$$\n",
        "\n",
        "Note that now we have two terms, because there are two paths that influence $h_1$ in the graph: \n",
        "\n",
        "$$\n",
        "J \\rightarrow a^{[2]}_1 \\rightarrow a^{[1]}_1 \\\\\n",
        "J \\rightarrow a^{[2]}_2 \\rightarrow a^{[1]}_1\n",
        "$$\n",
        "\n",
        "When we apply the chain rule, we need to consider both the paths. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_qGXj5GLCex",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 1):** Which two parameters should be used to calculate the partial derivative of $J(\\mathbf a^{[2]}, \\mathbf y)$ with respect to $a^{[1]}_2$?\n",
        "\n",
        "_Choose from: $\\Theta^{[2]}_{11}, \\Theta^{[2]}_{12}$; $\\ \\ \\ $ $\\Theta^{[2]}_{11}, \\Theta^{[2]}_{21}$;$\\ \\ \\ $ $\\Theta^{[2]}_{21}, \\Theta^{[2]}_{22}$;$\\ \\ \\ $ $\\Theta^{[2]}_{12}, \\Theta^{[2]}_{22}$_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W31q9r2jRgDK",
        "colab_type": "text"
      },
      "source": [
        "I think by now, you should have understood how back propagation works intuitively. But there is another thing you should pay attention to, $h_1$ is actually the activation after applying the nonlinearity, i.e., ReLU function. So $a^{[1]}_1 = R(s^{[1]}_1)$, and  $a^{[1]}_2 = R(s^{[1]}_2)$, and:\n",
        "\n",
        "$$\n",
        "s^{[1]}_1 = \\Theta^{[1]}_{11} x_1 + \\Theta^{[1]}_{21} x_2 + b^{[1]}_1\\\\\n",
        "s^{[1]}_2 = \\Theta^{[1]}_{12} x_1 + \\Theta^{[1]}_{22} x_2 + b^{[1]}_2\n",
        "$$\n",
        "\n",
        "So, don't forget to calculate the gradient $\\frac{\\partial a^{[1]}_1}{\\partial s^{[1]}_1}$ when you perform backpropagation. Since  $\\frac{\\partial a^{[1]}_1}{\\partial s^{[1]}_1}$ is dependent on what nonlinearity function you choose, you may think for your self **why people prefer to use ReLU rather than Sigmoid these days**. Recall the gradient of sigmoid function is given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) (1 - \\sigma(z))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOfx57VOOLv5",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 2):** Why people often prefer to use the ReLU function rather than Sigmoid function when choosing nonlinearity?\n",
        "\n",
        "_Replace with your answer_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-CV9_wstXly",
        "colab_type": "text"
      },
      "source": [
        "## 5 Techniques to improve training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtR2DKNg3IXr",
        "colab_type": "text"
      },
      "source": [
        "We now want to discuss some techniques that help train better models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lp9bL1E3ljn",
        "colab_type": "text"
      },
      "source": [
        "### .a Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q9KcThi2vHg",
        "colab_type": "text"
      },
      "source": [
        "The idea behind momentum is that the gradient, even so it might not point into the right direction, on average always points towards a proper good direction. This is especially true for SGD, but we'll stick to the normal gradient decent for this example. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K6DHRCPnEdL",
        "colab_type": "text"
      },
      "source": [
        "Going away from Neural Networks for a moment let's assume we want to find the minimum of the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function). Let's further assume we use gradient descent to find the minimum.  **Note:** This function is used as a benchmark for optimization algorithms, because it's known to be difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3n0A165Ond",
        "colab_type": "text"
      },
      "source": [
        "We define a starting point, the number of steps we want to use, and a learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AgaqZ7jfBt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "startingPoint = np.asarray([0.5,2])\n",
        "numberOfSteps = 100\n",
        "learningRate = 0.0003"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h1Jbmz45cIl",
        "colab_type": "text"
      },
      "source": [
        "Next we define the Rosenbrock function, for some given parameters $a, b$ and to save you some time we also defined the derivative of the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSwfs3VbfMEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a  = 1\n",
        "b  = 100\n",
        "f  = lambda x,y : (a-x)**2+b*(y-x**2)**2\n",
        "df = lambda x,y: np.asarray([4*b*x**3-4*b*x*y+2*x-2*a,2*b*(y-x**2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvBrdmZi5_8Q",
        "colab_type": "text"
      },
      "source": [
        "By now you should know how much we like to visualize things, so let's go ahead and try to visualize the steps of gradient descent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzsVsRWigPal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trace = [startingPoint]; # The first point of the trace is our starting point\n",
        "for step in range(numberOfSteps): # For each step\n",
        "\n",
        "  lastPoint = trace[-1]                          # We get the last point of our trace\n",
        "  update =  - df(lastPoint[0],lastPoint[1])      # We calculate the update\n",
        "  trace.append(lastPoint + learningRate *update) # We add a new point to our trace\n",
        "\n",
        "# Now we have a list of tuples, for plotting we need two arrays, with the x and y coordinates, respectively.\n",
        "x_trace,y_trace = np.asarray(trace)[:,0],np.asarray(trace)[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tahrVfzCU4Dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard figure definition\n",
        "fig = plt.figure(figsize=[10,5])\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlim(0,2)\n",
        "ax.set_ylim(-1,3)\n",
        "\n",
        "# Here we create the data to plot a contour of the underlying funciton\n",
        "X = np.arange(0, 2, 0.01)\n",
        "Y = np.arange(-1, 3, 0.01)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = f(X,Y)\n",
        "\n",
        "# The next line plots the contour\n",
        "ax.contour(X, Y, Z, levels=(np.asarray(range(0,50,1))/2)**3,cmap='cool')\n",
        "\n",
        "# And finally, we plot \n",
        "ax.plot(x_trace,y_trace)\n",
        "ax.set_title(\"Normal gradient decent\")\n",
        "ax.plot(x_trace,y_trace,c='blue')\n",
        "ax.annotate('Startpoint', xy=startingPoint)\n",
        "ax.annotate('Optimal', xy=[1,1])\n",
        "ax.scatter(x=[startingPoint[0],1],y=[startingPoint[1],1],zorder=10,c='green')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ1GkxgJB4Hd",
        "colab_type": "text"
      },
      "source": [
        "Oh... that doesn't look so good. The solutions jumps from one side to the other instead of going to the minimum.\n",
        "\n",
        "One way to fix this would be to start a better starting point or to change the learning rate. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhg4wwU3TnrV",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 3):** Try out some other learning rates and starting points. What do you observe?\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvj6W7geUBYA",
        "colab_type": "text"
      },
      "source": [
        "#### Programming : Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFHgBVngZh-_",
        "colab_type": "text"
      },
      "source": [
        "Now let's try to implement Momentum in it's most basic form. Momentum just means to reuse a part of the last update as a general direction to go to, and it can be written as follows:\n",
        "\n",
        "$$\n",
        "\\text{update}_{k+1} = \\beta \\cdot \\text{update}_k + \\nabla f(\\Theta_{k})\\\\ \n",
        "\\Theta_{k+1} = \\Theta_k + \\alpha \\cdot \\text{update}_{k+1}\n",
        "$$\n",
        "\n",
        "where $\\beta$ is a parameter for the momentum and $\\alpha$ is the normal learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJPILMpfl8dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell just resets the starting point and learning rate, \n",
        "# so that the following experiments behave the same for everybody.\n",
        "startingPoint = np.asarray([0.5,2])\n",
        "numberOfSteps = 100\n",
        "learningRate = 0.002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHUHayHtc_H4",
        "colab_type": "text"
      },
      "source": [
        "**Your turn (Question 4)**: Please implement the basic version of momentum in the cell below. In the end, `\n",
        "traceWithMomentum` should contain the coordinates of the trace as tuples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JuwRH8ZVg8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta = 0.0\n",
        "traceWithMomentum = [startingPoint]; # The first point of the trace is our starting point\n",
        "##########\n",
        "# Write your code here, usually about 6 lines of code.\n",
        "# You may find it useful to reference the previous tracing code block\n",
        "#\n",
        "#  Write your code \n",
        "#\n",
        "##########\n",
        "\n",
        "##########\n",
        "# The following code converts the list of tuples into two arrays, with the x and y coordinates, respectively.\n",
        "# We need this for plotting purposes.\n",
        "x_traceWithMomentum,y_traceWithMomentum = np.asarray(traceWithMomentum)[:,0], np.asarray(traceWithMomentum)[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acR3u0aKeCNF",
        "colab_type": "text"
      },
      "source": [
        "Then let's visualise your work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHQXG_h9W-M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard figure definition\n",
        "fig = plt.figure(figsize=[10,5])\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax1.set_xlim(0,2)\n",
        "ax1.set_ylim(-1,3)\n",
        "ax2.set_xlim(0,2)\n",
        "ax2.set_ylim(-1,3)\n",
        "\n",
        "# The next line plots the contour\n",
        "ax1.contour(X, Y, Z, levels=(np.asarray(range(0,50,1))/2)**3,cmap='cool')\n",
        "ax2.contour(X, Y, Z, levels=(np.asarray(range(0,50,1))/2)**3,cmap='cool')\n",
        "\n",
        "# And finally we plot \n",
        "ax1.set_title(\"Normal gradient descent\")\n",
        "ax1.plot(x_trace,y_trace,c='blue')\n",
        "ax1.annotate('Start Point', xy=startingPoint)\n",
        "ax1.annotate('Optimal Point', xy=[1,1])\n",
        "ax1.scatter(x=[startingPoint[0],1],y=[startingPoint[1],1],zorder=10,c='green')\n",
        "ax2.set_title(\"Gradient descent with a momentum (beta = {})\".format(beta))\n",
        "ax2.plot(x_traceWithMomentum,y_traceWithMomentum,c='blue')\n",
        "ax2.annotate('Start Point', xy=startingPoint)\n",
        "ax2.annotate('Optimal Point', xy=[1,1])\n",
        "ax2.scatter(x=[startingPoint[0],1],y=[startingPoint[1],1],zorder=10,c='green')\n",
        "plt.show()\n",
        "print(\"The distance to the optimal point is {:f} and {:f}, respectively.\".format(np.linalg.norm(trace[-1]-1),np.linalg.norm(traceWithMomentum[-1]-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLaangKOUKIL",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 5):** What happens for `beta`$ = 0.1$?\n",
        "\n",
        "_Choose from: We get a perfect result, The training gets closer, Everything stays the same, The result gets a bit worse, The result is terrible_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUxEzn6DURbx",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 6):** What happens for `beta`$ = 0.8$?\n",
        "\n",
        "_Choose from: We get a perfect result, The training gets closer, Everything stays the same, The result gets a bit worse, The result is terrible_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgDGQuiR1ZU0",
        "colab_type": "text"
      },
      "source": [
        "**Tip:** If you want to use momentum in in PyTorch, you just have to look at the documentation of the optimizer you use (e.g. type `?? optim.SGD` into a codeblock) \n",
        "\n",
        "**Tip:** If you want to learn more about momentum, we highly recommend checking out [this](https://distill.pub/2017/momentum/) Distill article. It also has a better version of our visualization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4uQczVrteCq",
        "colab_type": "text"
      },
      "source": [
        "### .b Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbiHsY8PS446",
        "colab_type": "text"
      },
      "source": [
        "In earlier lectures we learned about regularization and validation. In this section, we plan to put this into the context of neural networks. One of the benefits of neural networks is that they (theoretically) can learn anything. Unfortunatly, that makes them prone to overfitting. This effect is illustrated in the figure, below while the training error goes down epoch after epoch the Validation error starts to increase after a while. The best time to stop is highlighted with the arrow.\n",
        "\n",
        "The practice to not train a model until convergence of the training error, but to stop before is referred to as **early stopping**. You can use a validation set to decide when to stop. However, it's not as easy as it might look in the illustration below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3soEaITtNY6",
        "colab_type": "text"
      },
      "source": [
        "![Here should be the picture of a neural network](https://www.comp.nus.edu.sg/~mstrobel/pict/EarlyStopping.png)\n",
        "\n",
        "_(Diagram credit: Martin Strobel, NUS; CC BY 4.0)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFa-GtrVvjdr",
        "colab_type": "text"
      },
      "source": [
        "To illustrate the how early stopping works and why it's not a trivial thing to do, let's look at an example.  Let's create the following data set: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj3fPSC1itUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your success in this course does not depend on udnerstanding the following lines of code. \n",
        "# They just create an odd dataset (i.e., just run them)\n",
        "X_train = np.concatenate([np.concatenate([np.arange(-2.5,2.5,0.2).reshape([25,1]),0.1*np.ones([25,1])],axis=1),np.concatenate([np.arange(-2.5,2.5,0.2).reshape([25,1]),-0.1* np.ones([25,1])],axis=1)]) \n",
        "y_train  = np.ones(50)\n",
        "y_train[25:] = -1\n",
        "y_train[10] = -1\n",
        "y_train[11] = -1\n",
        "X_val = X_train.copy()\n",
        "X_val[:,0] = X_val[:,0] +  0.1\n",
        "y_val  = np.ones(50)\n",
        "y_val[25:] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4e2YEDywRSM",
        "colab_type": "text"
      },
      "source": [
        "We illustrate the data set below. You can see it's almost linearly separable. We just changed the label of two training points. You can think of this as noise or error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tHtMAP4JlGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is how the dataset looks like\n",
        "plt.title(\"An artificial data set\")\n",
        "plt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap=\"bwr\")\n",
        "plt.scatter(X_val[:,0],X_val[:,1],c=y_val,cmap=\"bwr\",marker=\"s\")\n",
        "plt.legend([\"Train\",\"Validation\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEomX4v8wiG7",
        "colab_type": "text"
      },
      "source": [
        "We use the same steps as above to create and train a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUSwm7q3yrUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: Please do not change this block of code as we need them for reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2MckoTGi7Tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we again transform our numpy arrays to Tensors\n",
        "X_train_T, X_val_T, y_train_T, y_val_T = torch.from_numpy(X_train),torch.from_numpy(X_val), torch.from_numpy(y_train), torch.from_numpy(y_val)\n",
        "# Here we again create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_T.float(),y_train_T.float().view(-1,1)), batch_size=100)\n",
        "validationloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_T.float(),y_val_T.float().view(-1,1)), batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFjFmwK55IBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the architecture we use for this experiment.\n",
        "netWeightDecay = nn.Sequential(\n",
        "    nn.Linear(2, 3),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(3,3),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(3,1),\n",
        "    nn.Sigmoid(),\n",
        ").float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ba9oY-c-WYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = optim.SGD(netWeightDecay.parameters(),lr=1e-1,weight_decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNA20wRe-lfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the first time we actually want to look at the loss in each epoch in detail, so we save it in two variables\n",
        "trainingLosses,validationLosses =  train(netWeightDecay,trainloader,validationloader,loss,opt,n_epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g_BV7bInWfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's look at the final result\n",
        "plotResults(netWeightDecay,X_train,y_train,X_val,y_val,title=\"Our first result\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCevqKw9xDSP",
        "colab_type": "text"
      },
      "source": [
        "As expected, the neural network overfits and so misclassifies some of the points in the validation set.\n",
        "\n",
        "Could we have solved the problem with early stopping? Let's take a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF8nWwxTYkgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"The training a validation loss in each epoch\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(trainingLosses)\n",
        "plt.plot(validationLosses)\n",
        "plt.legend([\"Training loss\",\"Validation loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg5EHhJ4U_XP",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 7):** Would it have been a good idea to stop the first time the validation error goes up?\n",
        "\n",
        "_Choose from: Yes, No_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Jjibq0VHEW",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 8):** What seems to be a good epoch to stop after (at least in this experiment)? You might want to run it more then once.\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS9-fzzUVPcT",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 9):** Did it work as expected? If not what seems to be the issue?\n",
        "\n",
        "_Replace with your answer_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivb5WI8Z0pQh",
        "colab_type": "text"
      },
      "source": [
        "This experiment is not ment to discourage you from using early stopping. It's mostly to show that in practice things often don't look as pretty as on lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZZ8b-wiVq8M",
        "colab_type": "text"
      },
      "source": [
        "### .c Weight Decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuMRV_Ru0_iB",
        "colab_type": "text"
      },
      "source": [
        "Now let's turn to the second trick of this section: _weight decay_. Weight decay is actually a method we already discussed. It's a technique to supress unnecessarily high weights in a network. It's (ignoring some technical details) the same as L2 regularization. \n",
        "\n",
        "You might have spotted the ```weight_decay``` parameter in the definition of the optimizer in the code above. Currently, it is set to 0. Can we solve the problem of the noisy data using weight decay?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9LV_zkhVyGH",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn (Question 10):** For what `weight_decay` value can we get a classifier that correctly classifies the points in the validation set?\n",
        "\n",
        "_Replace with your answer_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXFs7a9O5dAe",
        "colab_type": "text"
      },
      "source": [
        "This comes to the end of this notebook.  Please submit your work to the IVLE workbin for by the appropriate deadline.  Don't forget to do your pre-class video watching and work in the subsequent notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q41sH-S9sIBX",
        "colab_type": "text"
      },
      "source": [
        "# Further Resources\n",
        "\n",
        "* http://neuralnetworksanddeeplearning.com/index.html (30/07/18)\n",
        "* https://www.deeplearningbook.org/ (23/07/18)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kGaO2wXmyVP",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Credits\n",
        "Authored by Lee Yi Quan, Poh Jie, Mohammad Neamul Kabir, Martin Strobel, Liangming Pan, [Min-Yen Kan](http://www.comp.nus.edu.sg/~kanmy) (2019), affiliated with [WING](http://wing.comp.nus.edu.sg), [NUS School of Computing](http://www.comp.nus.edu.sg) and [ALSET](http://www.nus.edu.sg/alset).\n",
        "Licensed as: [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/ ) (CC BY 4.0).\n",
        "Please retain and add to this credits cell if using this material as a whole or in part.\n",
        "Other Credits (inclusive of photos): "
      ]
    }
  ]
}